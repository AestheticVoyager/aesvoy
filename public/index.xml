<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AesVoy</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on AesVoy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>© 2024 Mahan</copyright>
    <lastBuildDate>Fri, 09 Aug 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DeepFake Detection Methods</title>
      <link>http://localhost:1313/posts/deepfakedetection/</link>
      <pubDate>Fri, 09 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/deepfakedetection/</guid>
      <description>In this blog post, we explore the topic of image generators and their detection techniques. I&amp;rsquo;ll discuss various methods for detecting image generators and their manipulations. These include analyzing the visual content of an image, examining its metadata, and using machine learning algorithms to identify patterns in the data.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/deepfakedetection/featured.png" />
    </item>
    
    <item>
      <title>From CNNs to Vision Transformers: The Future of Image Recognition</title>
      <link>http://localhost:1313/posts/vit/</link>
      <pubDate>Thu, 08 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/vit/</guid>
      <description>Vision Transformers (ViTs) are redefining image recognition by using Transformer models to capture global context, unlike traditional Convolutional Neural Networks (CNNs) that focus on local features. ViTs excel with large datasets and show impressive scalability and performance.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/vit/featured.png" />
    </item>
    
    <item>
      <title>Misconceptions of Programming Paradigms</title>
      <link>http://localhost:1313/posts/progparadigms/</link>
      <pubDate>Wed, 07 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/progparadigms/</guid>
      <description>As a developer, you might have come across the misconception that writing code without classes in a language that supports Object-Oriented Programming (OOP) automatically makes it functional. In reality, this code is more likely procedural.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/progparadigms/featured.png" />
    </item>
    
    <item>
      <title>imageNet-Computer Vision Backbone</title>
      <link>http://localhost:1313/posts/imagenet/</link>
      <pubDate>Tue, 06 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/imagenet/</guid>
      <description>ImageNet is more than just a dataset. The sheer scale of ImageNet, combined with its detailed labeling, made it essentially the backbone of Computer Vision.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/imagenet/featured.png" />
    </item>
    
    <item>
      <title>Transformers &amp; Attention</title>
      <link>http://localhost:1313/posts/attention-transformer/</link>
      <pubDate>Mon, 05 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/attention-transformer/</guid>
      <description>This blog post explains how self-attention and softmax function in Transformer models, crucial for modern NLP. It breaks down how self-attention helps models understand relationships between tokens and how softmax ensures efficient computation and numerical stability.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/attention-transformer/featured.jpg" />
    </item>
    
    <item>
      <title>Diffusion VS Auto-Regressive Models</title>
      <link>http://localhost:1313/posts/diffusion-vs-auto-regressive/</link>
      <pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/diffusion-vs-auto-regressive/</guid>
      <description>Generative AI has come a long way, producing stunning images from simple text prompts. But how do Diffusion and Auto-Regressive models work, and why are diffusion models preferred.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/diffusion-vs-auto-regressive/featured.jpg" />
    </item>
    
    <item>
      <title>Mathematics of Risk</title>
      <link>http://localhost:1313/posts/black-scholes/</link>
      <pubDate>Sat, 03 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/black-scholes/</guid>
      <description>The Black-Scholes-Merton equation is a model for pricing options. This equation revolutionized finance by providing a precise method for determining fair option prices, improving risk management and trading efficiency.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/black-scholes/featured.jpg" />
    </item>
    
    <item>
      <title>PageRank</title>
      <link>http://localhost:1313/posts/pagerank/</link>
      <pubDate>Fri, 02 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/pagerank/</guid>
      <description>PageRank, created by Google founders Larry Page and Sergey Brin, changed the web by ranking pages based on the quality and quantity of their links, rather than just keywords. It evaluates a page’s authority through its endorsements, improving the relevance and trustworthiness of search results.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/pagerank/featured.jpg" />
    </item>
    
    <item>
      <title>Keynesian vs. Austrian vs. Monetarist Economics</title>
      <link>http://localhost:1313/posts/ecnomics/</link>
      <pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/ecnomics/</guid>
      <description>This article compares Keynesian, Austrian, and Monetarist economic theories, discussing their core principles, historical origins, and key figures. It highlights Austrian economics as the closest to libertarian values and examines the influence of these theories on modern global economic policies.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/ecnomics/featured.jpg" />
    </item>
    
    <item>
      <title>Diverse Roles in Data Science</title>
      <link>http://localhost:1313/posts/datanerd/</link>
      <pubDate>Mon, 29 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/datanerd/</guid>
      <description>In the rapidly evolving field of data science, several specialized roles have emerged to tackle various aspects of data management, analysis, and implementation. Among these roles, data scientists, data analysts, and data engineers are some of the most prominent.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/datanerd/featured.png" />
    </item>
    
    <item>
      <title>Context vs Content</title>
      <link>http://localhost:1313/posts/context/</link>
      <pubDate>Sun, 28 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/context/</guid>
      <description>In the bustling world of digital media, the term content is ubiquitous. While content creation is about generating material, context creation involves crafting the setting and conditions that allow the content to be meaningful and impactful.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/context/featured.jpg" />
    </item>
    
    <item>
      <title>AlexNet Revolution</title>
      <link>http://localhost:1313/posts/alexnet/</link>
      <pubDate>Fri, 26 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/alexnet/</guid>
      <description>In 2012, the field of artificial intelligence witnessed a seismic shift. The catalyst for this transformation was a deep learning model known as AlexNet.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/alexnet/featured.jpg" />
    </item>
    
    <item>
      <title>Generative Adversarial Network</title>
      <link>http://localhost:1313/posts/generative-adversarial-network/</link>
      <pubDate>Wed, 29 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/generative-adversarial-network/</guid>
      <description>A neural network is like a highly sophisticated, multi-layered calculator that learns from data. It consists of numerous “neurons” (tiny calculators) connected in layers, with each layer performing a unique function to help the network make predictions or decisions.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/generative-adversarial-network/featured.jpeg" />
    </item>
    
    <item>
      <title>Variational-Auto-Encoder</title>
      <link>http://localhost:1313/posts/variational-auto-encoder/</link>
      <pubDate>Tue, 28 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/variational-auto-encoder/</guid>
      <description>The beauty of VAEs lies in their ability to generate new samples by randomly sampling vectors from this known region and then passing them through the generator part of our model.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/variational-auto-encoder/featured.jpeg" />
    </item>
    
    <item>
      <title>Auto-Encoder</title>
      <link>http://localhost:1313/posts/auto-encoder/</link>
      <pubDate>Mon, 27 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/auto-encoder/</guid>
      <description>An autoencoder begins its journey by compressing input data into a lower dimension. It then endeavors to reconstruct the original input from this compressed representation.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/auto-encoder/featured.jpeg" />
    </item>
    
    <item>
      <title>Delaunay Triangulation</title>
      <link>http://localhost:1313/posts/delaunay-triangulation/</link>
      <pubDate>Sun, 26 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/delaunay-triangulation/</guid>
      <description>Delaunay triangulation is a process that takes a set of points in n-dimensional space as input and returns a network of triangles connecting these points. The resulting structure is called a triangulation or mesh.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/delaunay-triangulation/featured.jpg" />
    </item>
    
    <item>
      <title>Paltering</title>
      <link>http://localhost:1313/posts/paltering/</link>
      <pubDate>Sat, 25 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/paltering/</guid>
      <description>Paltering reminds us that even truthful statements can be used to mislead. Understanding the context and limitations of facts, and the role of scientific method in informing but not dictating actions, is essential.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/paltering/featured.jpg" />
    </item>
    
    <item>
      <title>Done Manifesto</title>
      <link>http://localhost:1313/posts/done-manifesto/</link>
      <pubDate>Wed, 22 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/done-manifesto/</guid>
      <description>In today&amp;rsquo;s fast-paced world, getting things done is crucial. The Done manifesto is all about embracing this mindset and applying it to your work.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/done-manifesto/featured.jpg" />
    </item>
    
    <item>
      <title>Weighted Voronoi Stippling</title>
      <link>http://localhost:1313/posts/weigthed-voronoi-stippling/</link>
      <pubDate>Mon, 20 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/weigthed-voronoi-stippling/</guid>
      <description>Stippling, a timeless artistic technique, traces its origins back through the annals of art history, where it emerged as a method of creating texture, depth, and form through the precise placement of dots.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/weigthed-voronoi-stippling/featured.jpg" />
    </item>
    
    <item>
      <title>Voyage through Hidden Markov Models</title>
      <link>http://localhost:1313/posts/hidden-markov-models/</link>
      <pubDate>Sun, 19 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/hidden-markov-models/</guid>
      <description>Hidden Markov Models (HMMs) are statistical models used for sequential data analysis, where underlying states are inferred from observed data. Employed in speech recognition, bioinformatics, and more.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/hidden-markov-models/featured.jpg" />
    </item>
    
    <item>
      <title>Voronoi Diagram</title>
      <link>http://localhost:1313/posts/voronoi-diagram/</link>
      <pubDate>Wed, 15 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/voronoi-diagram/</guid>
      <description>Voronoi diagrams, also known as Dirichlet tessellation or Thiessen polygons, are everywhere in nature. You’ve likely encountered them thousands of times, but perhaps didn’t know what they were called.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/voronoi-diagram/featured.jpg" />
    </item>
    
    <item>
      <title>Less is More Paper Review</title>
      <link>http://localhost:1313/posts/less-is-more/</link>
      <pubDate>Sun, 05 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/less-is-more/</guid>
      <description>Less is More: Parameter-Free Text Classification with Gzip offers a novel text classification method using gzip compression, eliminating manual parameter tuning.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/less-is-more/featured.jpeg" />
    </item>
    
    <item>
      <title>Difference of Gaussians(DoG) Algorithm</title>
      <link>http://localhost:1313/posts/difference-of-gaussians/</link>
      <pubDate>Sat, 04 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/difference-of-gaussians/</guid>
      <description>The Difference of Gaussians (DoG) algorithm is a technique in image processing used for edge detection and feature enhancement.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/difference-of-gaussians/featured.jpeg" />
    </item>
    
    <item>
      <title>Infini-Attention Paper Review</title>
      <link>http://localhost:1313/posts/infini-attention/</link>
      <pubDate>Fri, 03 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/infini-attention/</guid>
      <description>Infini-Attention introduces a novel approach to scaling Transformer models for infinitely long inputs while managing memory and computation.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/infini-attention/featured.jpeg" />
    </item>
    
    <item>
      <title>Softmax</title>
      <link>http://localhost:1313/posts/softmax/</link>
      <pubDate>Fri, 19 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/softmax/</guid>
      <description>Softmax stands as a pivotal component in neural network architectures, offering a means to convert raw scores into interpretable probabilities.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/softmax/featured.png" />
    </item>
    
    <item>
      <title>Dither</title>
      <link>http://localhost:1313/posts/dither/</link>
      <pubDate>Wed, 17 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/dither/</guid>
      <description>In the realm of digital image processing, dithering algorithms play a crucial role in reducing the color palette of an image while maintaining visual quality.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/dither/featured.png" />
    </item>
    
    <item>
      <title>Kuwahara</title>
      <link>http://localhost:1313/posts/kuwahara/</link>
      <pubDate>Wed, 17 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/kuwahara/</guid>
      <description>Kuwahara was the world&amp;rsquo;s first edge preserving de-noising image processing algorithm.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/kuwahara/featured.png" />
    </item>
    
  </channel>
</rss>
