
[{"content":"","date":"19 April 2024","externalUrl":null,"permalink":"/","section":"AesVoy","summary":"","title":"AesVoy","type":"page"},{"content":"","date":"19 April 2024","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"19 April 2024","externalUrl":null,"permalink":"/tags/ml/","section":"Tags","summary":"","title":"ML","type":"tags"},{"content":"","date":"19 April 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"19 April 2024","externalUrl":null,"permalink":"/tags/softmax/","section":"Tags","summary":"","title":"Softmax","type":"tags"},{"content":" Introduction # Over the past decade, several groundbreaking Neural Network models have emerged, reshaping the landscape of artificial intelligence and machine learning. Here\u0026rsquo;s a curated list of the most impactful models released during this period:\nAlexNet (2012):\nContribution: This model pioneered the application of deep convolutional neural networks (CNNs) in image classification tasks, demonstrating the potential of deep learning in large-scale visual recognition. Influence: Its success ignited widespread interest in deep learning research and laid the foundation for subsequent advancements in CNN architectures. GoogleNet (Inception) (2014):\nContribution: GoogleNet introduced inception modules to enhance computational efficiency in deep neural networks. It also popularized techniques like global average pooling and auxiliary classifiers. Influence: Its innovative architecture inspired the development of more efficient models and spurred research into model compactness and computational efficiency. VGGNet (2014):\nContribution: VGGNet emphasized the significance of network depth by employing a straightforward yet deep architecture composed of repeated 3x3 convolutional layers. Influence: Its depth-focused design motivated further exploration of deeper networks and influenced subsequent architectures aiming for improved performance through increased depth. Seq2Seq Models (2014):\nContribution: Seq2Seq models introduced the encoder-decoder architecture, enabling tasks such as machine translation, text summarization, and speech recognition. Influence: They revolutionized sequence modeling tasks and paved the way for attention mechanisms in neural networks. ResNet (2015):\nContribution: ResNet addressed the challenge of training very deep neural networks by introducing residual connections, which alleviated the vanishing gradient problem. Influence: It led to the development of extremely deep architectures and became a staple in state-of-the-art models. DenseNet (2016):\nContribution: DenseNet introduced dense connectivity patterns between layers, promoting feature reuse and facilitating gradient flow in deep neural networks. Influence: Its architecture inspired models prioritizing feature reuse and gradient flow, resulting in improvements in parameter efficiency and performance. Transformer (2017):\nContribution: The Transformer model revolutionized natural language processing (NLP) with its self-attention mechanism, enabling effective modeling of long-range dependencies in sequences. Influence: It catalyzed the development of transformer-based models that achieved state-of-the-art performance across various NLP tasks. BERT (2018):\nContribution: BERT introduced pre-training of contextualized word embeddings using large-scale unlabeled text corpora, enabling transfer learning for downstream NLP tasks. Influence: It spurred research in transfer learning and contextualized embeddings, leading to the creation of diverse pre-trained language models with numerous applications. EfficientNet (2019):\nContribution: EfficientNet proposed a scalable and efficient CNN architecture that achieved state-of-the-art performance across different resource constraints by balancing network depth, width, and resolution. Influence: It highlighted the importance of model scaling for efficient and effective neural network design, inspiring research into scalable architectures. GPT-2 (2019):\nContribution: GPT-2 introduced a large-scale transformer-based language model capable of generating coherent and contextually relevant text on a wide range of topics. Influence: It expanded the boundaries of language generation and showcased the capabilities of large-scale transformer models for natural language understanding and generation tasks. These models represent significant milestones in neural network research, each contributing unique advancements that have shaped the field and laid the groundwork for further innovation. Their interconnectedness underscores the iterative nature of deep learning research, where each advancement builds upon existing models to push the boundaries of what is possible.\nRole of Softmax in Model Architectures # While not all models explicitly use the softmax function, many rely on it as a vital component for tasks like classification, probability estimation, and sequence generation. Let\u0026rsquo;s explore how some of these models leverage and benefit from the softmax function:\nAlexNet:\nAlexNet typically employs softmax activation in its final layer to convert raw output scores into class probabilities for image classification tasks. After passing through convolutional and pooling layers, features are flattened and fed into a fully connected layer followed by softmax, yielding a probability distribution over classes. GoogleNet (Inception):\nAlthough GoogleNet (Inception) doesn\u0026rsquo;t directly utilize softmax in its inception modules, it often incorporates softmax in the final layer for classification. Inception modules generate feature maps, which are aggregated, processed, and then passed through a softmax layer to obtain class probabilities. VGGNet:\nSimilar to AlexNet, VGGNet typically employs softmax activation in its final layer for image classification. After multiple convolutional and pooling layers, flattened features are passed through fully connected layers followed by softmax to produce class probabilities. Seq2Seq Models:\nIn tasks like machine translation or text summarization, Seq2Seq models often employ softmax activation in the decoder to generate probability distributions over the vocabulary at each time step. Softmax is applied to output logits to obtain probabilities, aiding in selecting the most probable token. BERT:\nWhile BERT doesn\u0026rsquo;t use softmax during pre-training, it often utilizes softmax for fine-tuning on downstream tasks like text classification or named entity recognition. BERT\u0026rsquo;s output representations pass through a softmax layer to obtain probabilities over different classes or labels in these tasks. GPT-2:\nGPT-2 uses softmax activation in its output layer for text generation. At each time step, the model predicts the next token by applying softmax to logits produced by the final layer, generating a probability distribution over the vocabulary. In all cases, the softmax function plays a pivotal role in converting raw model outputs into interpretable probability distributions, facilitating tasks like classification, sequence generation, and language modeling. Additionally, softmax activations produce gradients crucial for training via backpropagation and stochastic gradient descent, making it integral to the optimization process.\nSoftmax Function and Its Relationship with Cross-Entropy Loss # Understanding the relationship between the softmax function and the cross-entropy loss function is crucial for classification tasks in neural networks. Let\u0026rsquo;s delve into this relationship using mathematical notation:\nSoftmax Function: The softmax function transforms a vector of real numbers into a probability distribution, commonly used in the output layer of neural networks for multi-class classification. It\u0026rsquo;s defined as: [ \\text{softmax}(\\mathbf{z})_i = \\frac{e^{z i}}{\\sum{j=1}^{K} e^{z_j}} ]\nWhere: ( \\mathbf{z} = [z_1, z_2, \u0026hellip;, z_K] ) is the input vector of raw output scores (logits). ( K ) is the number of classes. ( e ) represents Euler\u0026rsquo;s number (approximately 2.71828). ( \\text{softmax}(\\mathbf{z})_i ) denotes the probability of the ( i )-th class after applying softmax. Cross-Entropy Loss Function:\nThe cross-entropy loss measures the dissimilarity between the predicted probability distribution (obtained from softmax) and the true label distribution. For multi-class classification, it\u0026rsquo;s defined as: [ \\text{Cross-Entropy Loss} = - \\sum_{i=1}^{K} y_i \\log(\\hat{y}_i) ] Where: ( K ) is the number of classes. ( y_i ) is the true probability of the ( i )-th class (either 0 or 1). ( \\hat{y}_i ) is the predicted probability of the ( i )-th class obtained from softmax output. Relationship:\nThe softmax function computes predicted probabilities of each class, while the cross-entropy loss evaluates how closely these predicted probabilities match the true labels. During training, minimizing cross-entropy loss encourages the model to produce predicted probabilities aligning with the true label distribution, facilitating accurate predictions in classification tasks. Softmax Function Definition # The softmax function is a mathematical operation commonly used in machine learning and statistics to convert a vector of real numbers into a probability distribution. Its formula is:\n[ \\text{softmax}(\\mathbf{z})i = \\frac{e^{z_i}}{\\sum{j=1}^{K} e^{z_j}} ]\nWhere:\n( \\mathbf{z} = [z_1, z_2, \u0026hellip;, z_K] ) is the input vector. ( K ) denotes the number of elements in the vector. ( e ) represents Euler\u0026rsquo;s number (approximately 2.71828). ( \\text{softmax}(\\mathbf{z})_i ) represents the ( i )-th element of the output vector after applying softmax. The softmax function exponentiates each element of the input vector and normalizes these values by dividing them by the sum of all exponentials, ensuring the output vector sums to 1, thus forming a valid probability distribution.\nMathematical Properties of Softmax # The softmax function possesses several mathematical properties, making it a valuable tool in machine learning for multi-class classification tasks. These properties include:\nOutput as Probability Distribution:\nSoftmax transforms input into a probability distribution, with each element representing the probability of the corresponding class, facilitating interpretability. Normalization:\nIt normalizes input values to ensure output probabilities are well-defined and independent of input scale. Monotonicity:\nSoftmax is a monotonic transformation, ensuring increasing input values lead to higher corresponding output probabilities. Sensitivity to Input Differences:\nSoftmax amplifies differences between input values, with higher input values yielding higher output probabilities. Differentiability:\nSoftmax is differentiable everywhere, enabling efficient computation of gradients for optimization. Numerical Stability:\nSoftmax is designed to handle numerical instability associated with exponentiating large or small input values, aiding in numerical robustness during computation. These properties collectively make softmax a fundamental component in classification tasks, providing a means to convert raw scores into probabilities efficiently.\nWidespread Use of Softmax # Softmax enjoys widespread adoption due to several factors:\nOutput Interpretation: Softmax ensures neural network outputs represent probabilities, facilitating easy interpretation where each element denotes the probability of input belonging to a class.\nGradient-friendly: Softmax\u0026rsquo;s differentiability enables efficient computation of gradients, crucial for training neural networks using algorithms like stochastic gradient descent.\nNumerical Stability: Softmax handles numerical instability associated with exponentiation, mitigating issues like overflow or underflow.\nCompatibility with Cross-Entropy Loss: Softmax naturally pairs with cross-entropy loss in many classification tasks, simplifying optimization and promoting convergence during training.\nProbabilistic Representation: Softmax naturally represents model outputs as probability distributions, making it suitable for tasks requiring probabilistic interpretations like classification.\nAlternatives to Softmax # Several alternatives to softmax exist, each with unique advantages and disadvantages, catering to specific task requirements:\nSigmoid Function: Suitable for binary classification tasks but requires modifications for multi-class classification.\nLogistic Function: Extensible to multi-class classification through one-vs-all approach but may suffer from vanishing gradients.\nArcTan Function: Smooth and continuous but less commonly used for classification tasks.\nGaussian Function: Suitable for tasks with Gaussian output distributions but computationally expensive.\nSoftplus Function: Efficiently avoids vanishing gradients but outputs are not normalized.\nSparsemax Function: Encourages sparsity in output probabilities but requires careful hyperparameter tuning.\nMaxout Function: Generalizes ReLU for complex functions but is computationally expensive and prone to overfitting.\nThe choice of activation function depends on task requirements, data nature, and computational considerations, with softmax remaining a popular choice for its simplicity, interpretability, and compatibility with classification tasks.\nSummary # Softmax stands as a pivotal component in neural network architectures, offering a means to convert raw scores into interpretable probabilities. Its widespread use is attributed to its compatibility with training algorithms, numerical stability, and natural integration with loss functions. Understanding softmax and its properties is essential for effectively leveraging it in classification tasks, contributing to the advancement of machine learning and artificial intelligence.\n","date":"19 April 2024","externalUrl":null,"permalink":"/posts/softmax/","section":"Posts","summary":"Softmax stands as a pivotal component in neural network architectures, offering a means to convert raw scores into interpretable probabilities.","title":"Softmax","type":"posts"},{"content":"","date":"19 April 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"17 April 2024","externalUrl":null,"permalink":"/tags/dither/","section":"Tags","summary":"","title":"Dither","type":"tags"},{"content":" Introduction # In the realm of digital image processing, dithering algorithms play a crucial role in reducing the color palette of an image while maintaining visual quality. These algorithms distribute quantization errors across neighboring pixels, resulting in visually pleasing images with fewer colors. In this blog post, we\u0026rsquo;ll delve into the implementation of two popular dithering algorithms, Floyd-Steinberg and Atkinson, using the power of Numba for performance optimization.\nUnderstanding Dithering Algorithms # Before we delve into the code, let\u0026rsquo;s briefly understand the two dithering algorithms we\u0026rsquo;ll be exploring:\nFloyd-Steinberg Dithering: Developed by Robert W. Floyd and Louis Steinberg in 1976. Distributes quantization errors to neighboring pixels in a specific pattern. Produces sharp images with noticeable noise. Atkinson Dithering: Developed by Bill Atkinson in 1982. Similar to Floyd-Steinberg but distributes errors differently. Produces smoother images with less visible noise. Implementation with Numba # Now, let\u0026rsquo;s see how we can implement these dithering algorithms efficiently using Numba, a Just-In-Time compiler for Python code.\n@numba.jit(\u0026#34;f4[:,:,:](f4[:,:,:])\u0026#34;, nopython=True, nogil=True) def floyd_steinberg(image): Lx, Ly, Lc = image.shape for j in range(Ly): for i in range(Lx): for c in range(Lc): rounded = round(image[i,j,c]) err = image[i,j,c] - rounded image[i,j,c] = rounded if i\u0026lt;Lx-1: image[i+1,j,c] += (7/16)*err if j\u0026lt;Ly-1: image[i,j+1,c] += (5/16)*err if i\u0026gt;0: image[i-1,j+1,c] += (1/16)*err if i\u0026lt;Lx-1: image[i+1,j+1,c] += (3/16)*err return image @numba.jit(\u0026#34;f4[:,:,:](f4[:,:,:])\u0026#34;, nopython=True, nogil=True) def atkinson(image): frac = 8 Lx, Ly, Lc = image.shape for j in range(Ly): for i in range(Lx): for c in range(Lc): rounded = round(image[i,j,c]) err = image[i,j,c] - rounded image[i,j,c] = rounded if i\u0026lt;Lx-1: image[i+1,j,c] += err / frac if i\u0026lt;Lx-2: image[i+2,j,c] += err /frac if j\u0026lt;Ly-1: image[i,j+1,c] += err / frac if i\u0026gt;0: image[i-1,j+1,c] += err / frac if i\u0026lt;Lx-1: image[i+1,j+1,c] += err / frac if j\u0026lt;Ly-2: image[i,j+2,c] += err / frac return image Explanation of the Code # We utilize NumPy for numerical operations, PIL (Python Imaging Library) for image loading and saving, and Numba for JIT compilation to enhance performance. Both Floyd-Steinberg and Atkinson algorithms are implemented as Numba-jitted functions. The algorithms iterate through each pixel of the image, applying error diffusion to distribute quantization errors. Finally, the processed images are saved to disk. Results \u0026amp; Conclusion # By applying Floyd-Steinberg and Atkinson dithering algorithms to an input image, we\u0026rsquo;ve successfully reduced its color palette while preserving visual quality. The utilization of Numba for performance optimization ensures efficient processing, making these algorithms suitable for large-scale image manipulation tasks.\nExperimentation with different images and tweaking parameters can yield varying results, allowing for customization based on specific requirements. Dithering algorithms continue to be relevant in various applications, including digital art, printing, and image compression.\nIn conclusion, by exploring dithering algorithms such as Floyd-Steinberg and Atkinson and leveraging the power of Numba for implementation, we\u0026rsquo;ve gained insights into enhancing image processing tasks with efficient and optimized code.\nLink to Complete Implementation in GitHub\n","date":"17 April 2024","externalUrl":null,"permalink":"/posts/dither/","section":"Posts","summary":"In the realm of digital image processing, dithering algorithms play a crucial role in reducing the color palette of an image while maintaining visual quality.","title":"Dither","type":"posts"},{"content":"","date":"17 April 2024","externalUrl":null,"permalink":"/tags/filter/","section":"Tags","summary":"","title":"Filter","type":"tags"},{"content":"","date":"17 April 2024","externalUrl":null,"permalink":"/tags/fitler/","section":"Tags","summary":"","title":"Fitler","type":"tags"},{"content":"","date":"17 April 2024","externalUrl":null,"permalink":"/tags/image-processing/","section":"Tags","summary":"","title":"Image Processing","type":"tags"},{"content":"","date":"17 April 2024","externalUrl":null,"permalink":"/tags/kuwahara/","section":"Tags","summary":"","title":"Kuwahara","type":"tags"},{"content":"The Kuwahara filter is a non-linear smoothing filter used in image processing for adaptive noise reduction. Most filters that are used for image smoothing are linear low-pass filters that effectively reduce noise but also blur out the edges. However the Kuwahara filter is able to apply smoothing on the image while preserving the edges.\nIt is named after Michiyoshi Kuwahara, Ph.D., who worked at Kyoto and Osaka Sangyo Universities in Japan, developing early medical imaging of dynamic heart muscle in the 1970s and 80s.\nKuwahara Filter description # The Kuwahara filter works on a window divided into 4 overlapping sub-windows. In each sub-window, the mean and variance are computed.\nThe output value (located at the center of the window) is set to the mean of the sub-window with the smallest variance.\nApplications # Originally the Kuwahara filter was proposed for use in processing RI-angiocardiographic images of the cardiovascular system.\nThe fact that any edges are preserved when smoothing makes it especially useful for feature extraction and segmentation and explains why it is used in medical imaging.\nThe Kuwahara filter however also finds many applications in artistic imaging and fine-art photography due to its ability to remove textures and sharpen the edges of photographs. The level of abstraction helps create a desirable painting-like effect in artistic photographs especially in the case of the colored image version of the filter. These applications have known great success and have encouraged similar research in the field of image processing for the arts.\nAlthough the vast majority of applications have been in the field of image processing there have been cases that use modifications of the Kuwahara filter for machine learning tasks such as clustering.\nThe Kuwahara filter has been implemented in CVIPtools.\nAnisotropic Kuwahara Filtering with Polynomial Weighting Functions Paper # The Anisotropic Kuwahara Paper link\nKuwahara was the world\u0026rsquo;s first edge preserving de-noising image processing algorithm. It was upgraded by the \u0026ldquo;Anisotropic Kuwahara Filtering with Polynomial Weighting Functions\u0026rdquo; paper, by:\nUpgraded by using a circular kernel instead of Box kernel. Instead of using naive weights, we use gaussian weights. This new formula: 1/(1+std_div), sector color = Ki, K(x)=(sum of Ki * Wi)/(sum of weights i) This removes indeterminate behavior and removes all conditional logic of the old algorithm. All these changes were made by Guiseppe Papari.\nThankfully we can just ditch the Gauss and instead approximate the weight using \u0026ldquo;Polynomials\u0026rdquo;.\nThen we\u0026rsquo;ll calculate the Eigen-Values. To calculate the Eigen-Values of the structure tensor and use them to calculate the eigenvectors that points in the direction of the minimum rate of change. We\u0026rsquo;re just essentially figuring out what direction a pixel points in using the eigenvector information.\nThe filter kernel can now angle itself and stretch itself to better fit image details and edges.\nThis new filter is called Anisotropic Kuwahara Filter.\nRecommendation: In-order to achieve High Contrast Visuals, it is better to apply the anisotropic kuwahara then apply the dither effect.\nMy Personal Optimized Implementation of Kuwahara filter # Personal Implementation\n","date":"17 April 2024","externalUrl":null,"permalink":"/posts/kuwahara/","section":"Posts","summary":"Kuwahara was the world\u0026rsquo;s first edge preserving de-noising image processing algorithm.","title":"Kuwahara","type":"posts"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]