
[{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/","section":"AesVoy","summary":"","title":"AesVoy","type":"page"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/cogan/","section":"Tags","summary":"","title":"CoGAN","type":"tags"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/dcgan/","section":"Tags","summary":"","title":"DCGAN","type":"tags"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/decoder/","section":"Tags","summary":"","title":"Decoder","type":"tags"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/encoder/","section":"Tags","summary":"","title":"Encoder","type":"tags"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/gan/","section":"Tags","summary":"","title":"GAN","type":"tags"},{"content":" Typical Neural Network Architecture vs. GAN Architecture # A neural network is like a highly sophisticated, multi-layered calculator that learns from data. It consists of numerous \u0026ldquo;neurons\u0026rdquo; (tiny calculators) connected in layers, with each layer performing a unique function to help the network make predictions or decisions. These neurons communicate with each other using numerical data and mathematical operations, akin to a game of telephone where each neuron modifies the information before passing it along. In contrast, a Generative Adversarial Network (GAN) operates like two secretive collaborators working together. One collaborator, the \u0026ldquo;counterfeiter\u0026rdquo; (or generator), attempts to create new data that appears real and authentic, much like a master forger trying to produce convincing fake art. The other collaborator, the \u0026ldquo;cop\u0026rdquo; (or discriminator), evaluates the counterfeiter\u0026rsquo;s creations to determine if they are genuine or fake. This process continues in a loop, with the counterfeiter improving its ability to generate realistic data and the cop getting better at detecting fakes. Unlike a single neural network that functions independently, a GAN consists of two parts working in tandem, often in a competitive manner. In practice, a GAN\u0026rsquo;s generator creates fake images, which are then mixed with real images. The discriminator randomly selects an image from this mix to determine whether it is real or generated. Based on the discriminator\u0026rsquo;s accuracy, both the generator and discriminator are adjusted. After numerous iterations, the generator becomes proficient at producing realistic images. Both networks in this scenario are multi-layer perceptrons (MLPs), typically used for simpler problems. However, MLPs can be combined to tackle more complex tasks, though this approach is not highly efficient.\nDCGAN # In 2015, researchers Alec Radford and Luke Metz proposed using more complex networks instead of simple ones to construct an even more sophisticated network. This led to the creation of Deep Convolutional GANs (DCGANs), which utilize convolutional neural networks instead of MLPs. This approach demonstrated improvements in generating realistic data.\nCoGAN # Around the same time, Couple GANs (CoGANs) were introduced, employing two pairs of generators and discriminators. In this setup, two simultaneous games occur during each training round. The generators share information but tweak their outputs to fool their respective discriminators. This results in generators capable of producing images with slight variations, such as a person with different hair colors or with and without glasses. Despite these advancements, GANs still struggled with generating high-quality images, often producing blurry and low-resolution results due to the discriminator\u0026rsquo;s tendency to detect fakes more easily at higher resolutions.\nProgressively Growing GAN # In 2017, NVIDIA researchers introduced Progressive Growing of GANs (PGGAN), a technique that significantly improved GAN capabilities and image quality. Traditional GANs have fixed architectures, leading to limitations in capacity and training stability. PGGANs address these issues by gradually increasing the size of both the generator and discriminator networks during training, enhancing their ability to learn complex patterns and maintaining stable training.\nHow PGGAN Works # Initial setup: Start with a small generator and discriminator. Progressive growth: Incrementally add layers to both networks. Training: Continue training with the same loss functions as traditional GANs. Benefits of PGGAN # Better image quality: Generates more realistic and diverse images. Increased resolution: Produces high-resolution images (e.g., 1024x1024 pixels). Improved stability: Ensures stable training throughout the process. Style-Based GANs # In 2018, NVIDIA researchers introduced Style-based GANs (SGANs), designed to generate high-quality images with the ability to manipulate their style while maintaining content consistency. Traditional GANs often produce images with a fixed style, which may not match the desired outcome. SGANs overcome this by allowing for more control over the generated image\u0026rsquo;s style.\nHow SGANs Work # SGANs consist of two main components:\nGenerator: Takes a random noise vector and generates an image with a specific style. Style encoder: Extracts style information from a reference image to manipulate the generated output. By separating content from style, SGANs provide more flexibility and control in image generation.\nBenefits of SGANs # More control: Enables precise manipulation of images while preserving their style. Improved quality: Generates high-quality, diverse images. Flexibility: Creates new images that are variations or combinations of existing ones. Real-World Applications # SGANs have been used in various domains, including:\nComputer vision: Tasks like image-to-image translation, data augmentation, and style transfer. Artistic creation: Generating realistic images with specific styles or creatively manipulating existing images. Recommendation # If you\u0026rsquo;ve not yet tried or know of This person does not exist, I highly recommend checking it out. Not only for the fun of it, but also for seeing a GAN in action.\n","date":"29 May 2024","externalUrl":null,"permalink":"/posts/generative-adversarial-network/","section":"Posts","summary":"In practice, a GAN\u0026rsquo;s generator creates fake images, which are then mixed with real images. The discriminator randomly selects an image from this mix to determine whether it is real or generated. Based on the discriminator\u0026rsquo;s accuracy, both the generator and discriminator are adjusted. After numerous iterations, the generator becomes proficient at producing realistic images.","title":"Generative Adversarial Network","type":"posts"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/learning/","section":"Tags","summary":"","title":"Learning","type":"tags"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/ml/","section":"Tags","summary":"","title":"ML","type":"tags"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/pggan/","section":"Tags","summary":"","title":"PGGAN","type":"tags"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/sgan/","section":"Tags","summary":"","title":"SGAN","type":"tags"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/vector/","section":"Tags","summary":"","title":"Vector","type":"tags"},{"content":"","date":"28 May 2024","externalUrl":null,"permalink":"/tags/ae/","section":"Tags","summary":"","title":"AE","type":"tags"},{"content":"","date":"28 May 2024","externalUrl":null,"permalink":"/tags/auto-encoder/","section":"Tags","summary":"","title":"Auto-Encoder","type":"tags"},{"content":"","date":"28 May 2024","externalUrl":null,"permalink":"/tags/compression/","section":"Tags","summary":"","title":"Compression","type":"tags"},{"content":"","date":"28 May 2024","externalUrl":null,"permalink":"/tags/supervised-learning/","section":"Tags","summary":"","title":"Supervised Learning","type":"tags"},{"content":"","date":"28 May 2024","externalUrl":null,"permalink":"/tags/vae/","section":"Tags","summary":"","title":"VAE","type":"tags"},{"content":"","date":"28 May 2024","externalUrl":null,"permalink":"/tags/variational-auto-encoder/","section":"Tags","summary":"","title":"Variational-Auto-Encoder","type":"tags"},{"content":" The Magic of Variational Auto-Encoders: Unleashing the Power of Continuous Representation # In the world of deep learning, Auto-Encoders (AE) have long been a staple in our quest to understand and generate complex data distributions. By encoding and decoding input data, AE models can learn compact representations that capture essential features of the underlying distribution. This process is akin to compressing an image into a smaller format, such as JPEG, which retains most of the original information while reducing its size.\nIn essence, Auto-Encoders are neural networks composed of two main components: the encoder and the decoder. The encoder takes in input data, transforms it into a lower-dimensional representation (also known as the bottleneck or latent space), and then passes this compacted information to the decoder. The decoder, on the other hand, uses this compressed representation to reconstruct the original input data.\nThis process of encoding and decoding allows AE models to learn meaningful representations that can be used for various tasks such as dimensionality reduction, anomaly detection, and generative modeling. By minimizing the reconstruction error between the input data and its reconstructed version, Auto-Encoders are able to identify patterns and relationships within the data that would otherwise remain hidden.\nThe Limitations of Traditional Auto-Encoders # While traditional Auto-Encoders have been incredibly successful in various applications, they do come with some limitations. One major drawback is their inability to generate new samples from the learned representation. This is because AE models are designed primarily for reconstruction and not generation. When we try to sample vectors randomly from the latent space, we\u0026rsquo;re essentially \u0026ldquo;blindfolded\u0026rdquo; without any prior knowledge of where these vectors lie within the distribution.\nThis limitation becomes particularly problematic when we want to generate novel images or samples that are coherent with the learned representation. Traditional Auto-Encoders simply aren\u0026rsquo;t designed for this task, and their generated outputs often lack the desired level of realism and diversity.\nVariational Auto-Encoders: The Game-Changer # Enter Variational Auto-Encoders (VAEs), the game-changing innovation that solves this very problem. By defining a region or pool of vectors from which we want to sample, VAEs can learn to constrain their representation within this universe. This is achieved during the training phase by optimizing the model\u0026rsquo;s parameters to find these pools.\nThe beauty of VAEs lies in their ability to generate new samples by randomly sampling vectors from this known region and then passing them through the generator part (or deconvolutional layer) of our model. The resulting images are not only realistic but also continuous, allowing us to subtly alter the vector\u0026rsquo;s values to produce novel yet valid-looking outputs.\nTo illustrate this concept, let\u0026rsquo;s consider a VAE trained on handwritten digits from 0 to 9. During training, the model learns to identify distinct pools or regions that represent each digit (e.g., pool for \u0026ldquo;0\u0026rdquo;, pool for \u0026ldquo;1\u0026rdquo;, etc.). These pools are learned within a continuous region, allowing us to sample vectors and generate new images by perturbing these values.\nThe implications of this approach are profound. By sampling from the same continuous region, we can create an infinite variety of generated images that appear natural and coherent when placed next to each other. This property is particularly useful in applications where data generation is crucial, such as image synthesis or text-to-image translation.\nAuto-Encoders vs Variational Auto-Encoders: A Comparison # While traditional Auto-Encoders have their strengths, they are limited by their inability to generate new samples from the learned representation. In contrast, Variational Auto-Encoders offer a powerful tool for generating novel images while maintaining control over the sampling process.\nHere\u0026rsquo;s a summary of the key differences between these two approaches:\nPurpose: Traditional AE models focus on reconstruction and dimensionality reduction, whereas VAEs are designed specifically for generative modeling. Sampling: AEs rely on random sampling from the latent space, which can lead to unpredictable results. VAEs, on the other hand, learn to sample vectors from a specific region or pool, allowing for more controlled generation of new samples. Representation: Traditional AE models typically use a fixed-size representation (latent space), whereas VAEs learn a continuous and probabilistic representation that allows for sampling and perturbation. Conclusion # In conclusion, Variational Auto-Encoders offer a powerful tool for generating novel images while maintaining control over the sampling process. By learning to sample vectors from specific regions or pools, we can unlock the secrets of continuous generation and create realistic and fascinating outputs.\n","date":"28 May 2024","externalUrl":null,"permalink":"/posts/variational-auto-encoder/","section":"Posts","summary":"The beauty of VAEs lies in their ability to generate new samples by randomly sampling vectors from this known region and then passing them through the generator part of our model. The resulting images are not only realistic but also continuous, allowing us to subtly alter the vector\u0026rsquo;s values to produce novel yet valid-looking outputs.","title":"Variational-Auto-Encoder","type":"posts"},{"content":" Understanding Autoencoders: Simplifying Unsupervised Learning # Autoencoders, in their simplest form, are neural networks designed to achieve two primary objectives: compression and reconstruction. But what does this mean, and why are they significant in the realm of machine learning? Let\u0026rsquo;s set sail en voyage into the core concepts of autoencoders, demystify their workings, and explore their practical applications.\n1. Compression and Reconstruction # An autoencoder begins its journey by compressing input data into a lower dimension. It then endeavors to reconstruct the original input from this compressed representation. The crux of its functionality lies in minimizing the difference between the attempted recreation and the original input, known as the reconstruction error.\nReconstruction Error = Reconstructed - Original\nThrough training, the autoencoder learns to exploit the inherent structure within the data to find an efficient lower-dimensional representation.\n2. The Role of the Encoder # The left part of the autoencoder, known as the encoder, plays a pivotal role. Its task is to transform the original input into a lower-dimensional representation. This process might sound complex, but it essentially involves mapping the data from its full input space into a lower-dimensional coordinate system that captures the underlying structure of the data.\n3. Understanding Data Structure # Real-world data often exhibits structure, meaning it doesn\u0026rsquo;t occupy the entirety of its input space but rather exists within a constrained subspace. For example, if we consider pairs like (Tokyo, Japan) or (Paris, France), while theoretically, combinations like (Hong Kong, Spain) are possible, they\u0026rsquo;re rarely observed in actual data. This constrained nature of data motivates the need for compression into a lower dimension.\n4. The Decoder\u0026rsquo;s Task # Once the data is compressed, the decoder steps in to reverse the encoding process, aiming to reconstruct the original input. Despite working with fewer dimensions, the decoder endeavors to recreate the higher-dimensional input as accurately as possible. This process introduces information loss, which is essential for effective learning within the autoencoder.\n5. Enforcing Information Loss # The middle layer of the autoencoder serves as a bottleneck, forcing information loss and compelling the network to find the most efficient way to condense input data into a lower dimension. Without this enforced information loss, the network could resort to trivial solutions, rendering it ineffective.\n6. Denoising Autoencoders: A Clever Tweak # To avoid trivial solutions, such as merely multiplying the input by one, denoising autoencoders come into play. Before passing input into the network, noise is added to it, such as blur in the case of images. The network then learns to remove this added noise and reconstruct the original input, thereby preventing trivial solutions and enhancing the learning process.\nPractical Applications of Autoencoders # Feature Extraction: After training, the encoder can be used to transform raw data into a new coordinate system, where similar records are clustered together. Anomaly Detection: By utilizing the reconstruction error as an anomaly score, autoencoders can detect anomalies in data that deviate from the normal structure. Missing Value Imputation: Autoencoders can be trained to predict missing values in data, enabling efficient data imputation. In conclusion, autoencoders are powerful tools in unsupervised learning, offering insights into data structure, dimensionality reduction, and information representation. By understanding their principles and applications, we can leverage autoencoders to unlock valuable insights from complex datasets.\n","date":"27 May 2024","externalUrl":null,"permalink":"/posts/auto-encoder/","section":"Posts","summary":"An autoencoder begins its journey by compressing input data into a lower dimension. It then endeavors to reconstruct the original input from this compressed representation. The crux of its functionality lies in minimizing the difference between the attempted recreation and the original input, known as the reconstruction error.","title":"Auto-Encoder","type":"posts"},{"content":"","date":"26 May 2024","externalUrl":null,"permalink":"/tags/cg/","section":"Tags","summary":"","title":"CG","type":"tags"},{"content":"","date":"26 May 2024","externalUrl":null,"permalink":"/tags/computational-geometry/","section":"Tags","summary":"","title":"Computational Geometry","type":"tags"},{"content":" Delaunay Triangulation: A Fundamental Concept in Computational Geometry # Delaunay triangulation is a fundamental concept in computational geometry and has numerous applications across various fields, including computer graphics, geographic information systems (GIS), engineering, and data analysis. In this article, we will delve into the world of Delaunay triangulations, exploring their definition, properties, and significance.\nWhat is Delaunay Triangulation? # Delaunay triangulation is a process that takes a set of points in n-dimensional space (n-D) as input and returns a network of triangles connecting these points. The resulting structure is called a triangulation or mesh. Each triangle is formed by three vertices, which are the closest neighbors to each other.\nThe Delaunay criterion for forming a triangle is based on the concept of circumcircles. A circumcircle is a circle that passes through all three vertices of a triangle. In a Delaunay triangulation, triangles are formed only when the circumcircle contains no other points from the input set within its interior.\nProperties and Applications # Delaunay triangulations have several desirable properties:\nConvex Hull: The resulting mesh is guaranteed to contain all the input points. Simplex: Each triangle in the mesh has a unique orientation, ensuring that there are no duplicate triangles or holes. Efficient Computation: Delaunay triangulation algorithms have been optimized for efficient computation and can handle large datasets. The applications of Delaunay triangulations are diverse:\nComputer Graphics: Triangulating 2D or 3D points enables the creation of smooth surfaces, meshes, and animations. GIS: Delaunay triangulation is used in GIS to create maps with accurate boundaries, calculate distances between locations, and perform spatial analysis. Engineering: The technique is employed in various engineering fields, such as structural mechanics (e.g., stress analysis), fluid dynamics, and robotics. Data Analysis: Delaunay triangulations can be used for data visualization, clustering, and dimensionality reduction. Relationship with Voronoi Diagrams # Interestingly, the dual of a Delaunay triangulation is a Voronoi diagram. This duality highlights the complementary nature of these two fundamental concepts in computational geometry. While Voronoi diagrams partition space into regions based on proximity to points, Delaunay triangulations connect those same points with triangles.\nConclusion # Delaunay triangulation is a powerful tool for analyzing and visualizing data in various fields. Its properties and applications make it an essential concept in the realm of computational geometry. By understanding how Delaunay triangulations work, developers can create more efficient algorithms, improve spatial analysis capabilities, and unlock new insights from complex datasets.\n","date":"26 May 2024","externalUrl":null,"permalink":"/posts/delaunay-triangulation/","section":"Posts","summary":"Delaunay triangulation is a process that takes a set of points in n-dimensional space as input and returns a network of triangles connecting these points. The resulting structure is called a triangulation or mesh.","title":"Delaunay Triangulation","type":"posts"},{"content":"","date":"26 May 2024","externalUrl":null,"permalink":"/tags/graphics/","section":"Tags","summary":"","title":"Graphics","type":"tags"},{"content":"","date":"26 May 2024","externalUrl":null,"permalink":"/tags/triangulation/","section":"Tags","summary":"","title":"Triangulation","type":"tags"},{"content":"","date":"26 May 2024","externalUrl":null,"permalink":"/tags/voronoi/","section":"Tags","summary":"","title":"Voronoi","type":"tags"},{"content":"","date":"25 May 2024","externalUrl":null,"permalink":"/tags/content/","section":"Tags","summary":"","title":"Content","type":"tags"},{"content":"","date":"25 May 2024","externalUrl":null,"permalink":"/tags/context/","section":"Tags","summary":"","title":"Context","type":"tags"},{"content":"","date":"25 May 2024","externalUrl":null,"permalink":"/tags/mislead/","section":"Tags","summary":"","title":"Mislead","type":"tags"},{"content":"","date":"25 May 2024","externalUrl":null,"permalink":"/tags/paltering/","section":"Tags","summary":"","title":"Paltering","type":"tags"},{"content":" Paltering: The Subtle Art of Misleading with the Truth # Facts, on their own, tell us nothing. It\u0026rsquo;s the context in which these facts are presented that gives them meaning and allows us to construct narratives. In philosophical terms, fact-checking is a necessary condition for telling a true story, but it\u0026rsquo;s not sufficient. This idea is crucial when we delve into the concept of paltering.\nPaltering is the act of misleading by telling the truth. Unlike lying, where false information is provided, paltering involves selecting truthful statements that lead someone to a false or misleading conclusion. It\u0026rsquo;s a subtle and sophisticated form of deception, often used to manipulate without directly falsifying information.\nContext and Facts # Let\u0026rsquo;s consider how context affects facts. When you present facts within a particular framework, you shape the story they tell. For example, stating that \u0026ldquo;crime rates have dropped by 20%\u0026rdquo; might seem positive. However, if the overall crime rate was very high to begin with, a 20% drop may still indicate a serious problem. Thus, the context in which we place facts transforms them into a narrative.\nIn philosophical terms, this is akin to the principle that fact-checking alone doesn\u0026rsquo;t ensure truthfulness. You need the right context and interpretation to form a true story. This concept is deeply rooted in the works of philosophers like David Hume, particularly his ideas about the is-ought problem.\nHume\u0026rsquo;s Guillotine and the Is-Ought Problem # David Hume, an 18th-century Scottish philosopher, introduced the is-ought problem, also known as Hume\u0026rsquo;s Guillotine. He argued that you cannot derive an \u0026ldquo;ought\u0026rdquo; from an \u0026ldquo;is.\u0026rdquo; In other words, you can\u0026rsquo;t infer what should be done based on what is, without introducing some additional assumptions.\nFor instance, just because science tells us how the world is, it doesn\u0026rsquo;t inherently tell us what we should do about it. Science describes phenomena, but it doesn\u0026rsquo;t prescribe actions. This gap between descriptive statements (what is) and prescriptive statements (what ought to be) is critical. It underscores the need for assumptions, values, or goals to bridge the two.\nThe Role of Scientific Method # But what if we tried to replace these assumptions with the scientific method? What if we used empirical data and experiments to determine what works? Surely, we can trust science to guide our actions, right?\nThe answer is nuanced. While the scientific method is a powerful tool for understanding the world, it is not infallible. It relies on rigorous testing, peer review, and reproducibility to validate findings. However, even with these mechanisms in place, science itself doesn\u0026rsquo;t dictate what we should do with the knowledge it provides. It can inform decisions, but it cannot make value judgments.\nFor instance, scientific research might show that a particular policy reduces pollution. However, whether society prioritizes this reduction over economic growth is a value judgment, not a scientific one. This is where the assumptions and goals we introduce play a crucial role.\nTrusting Science with a Critical Eye # So, can we trust science? Yes, as long as we remain critical and aware of its limitations. Science can guide us toward effective solutions, but we must be cautious about assuming it has all the answers or that it can make ethical decisions for us. Scientific findings should inform our choices, but they must be interpreted and applied within a broader context that includes ethical, social, and practical considerations.\nIn conclusion # In conclusion, paltering reminds us that even truthful statements can be used to mislead. Understanding the context and limitations of facts, and the role of scientific method in informing but not dictating actions, is essential. This awareness helps us navigate the complex interplay between truth, context, and the stories we construct from them.\n","date":"25 May 2024","externalUrl":null,"permalink":"/posts/paltering/","section":"Posts","summary":"Paltering reminds us that even truthful statements can be used to mislead. Understanding the context and limitations of facts, and the role of scientific method in informing but not dictating actions, is essential.","title":"Paltering","type":"posts"},{"content":"","date":"22 May 2024","externalUrl":null,"permalink":"/tags/done/","section":"Tags","summary":"","title":"Done","type":"tags"},{"content":" The Power of Completion # In today\u0026rsquo;s fast-paced world, getting things done is crucial. The Done manifesto is all about embracing this mindset and applying it to your work.\n12 Principles for Getting Things Done: # Know, Act, Complete: Recognize that there are only three states - not knowing, taking action, or being complete. Draft Mode: Accept that everything is a draft, even if you\u0026rsquo;re not sure what the final product will look like. No Editing Required: Don\u0026rsquo;t get bogged down in perfectionism - just finish! Fake It Till You Make It: Pretend you know what you\u0026rsquo;re doing (even if you don\u0026rsquo;t) and take action anyway. Procrastination is a Killer: If an idea takes more than a week to complete, it\u0026rsquo;s probably not worth pursuing. The End Justifies the Means: Focus on completing tasks rather than getting stuck in perfectionism. Let Go of Perfection: Once you\u0026rsquo;ve completed something, let go and move on - no attachment necessary! Laugh at Perfectionism: Recognize that striving for perfection is a waste of time and energy. Get Your Hands Dirty: People who don\u0026rsquo;t take action are missing out - get involved and make things happen! Failure is an Option (and Opportunity): View failure as a chance to learn and improve, rather than something to be feared. Destruction is a variant of done: Sometimes the best way to move forward is by tearing down old systems or ideas that are no longer serving you. Share Your Work: Publishing your work online counts as \u0026ldquo;done\u0026rdquo; - share it with others and take pride in what you\u0026rsquo;ve accomplished! By embracing these principles, you\u0026rsquo;ll be able to overcome procrastination, perfectionism, and other obstacles that hold people back from achieving their goals.\nOrigin of Done Manifesto # The Done manifesto is based on the principles of Lean Software Development, which emphasizes the importance of delivering value early and often, and continuously improving the product based on feedback from customers. It also emphasizes the importance of collaboration and communication among team members, as well as a focus on experimentation and iteration to improve the product over time.\nThe Done manifesto is not just about software development, but can be applied to any field where teams work together to create products or services that meet customer needs. By following the principles of the Done manifesto, teams can create high-quality products that deliver value to customers and promote continuous improvement through experimentation and iteration.\nTry It Out # Now that you\u0026rsquo;ve read through the Cult of Done manifesto, we invite you to try out some aspects of this mindset for yourself.\nChoose one principle that resonates with you the most (e.g. #5 Procrastination is a Killer) and challenge yourself to apply it in your daily life. Identify an area where you tend to procrastinate or get stuck in perfectionism, and commit to completing something small but meaningful within the next week. Share one of your completed projects with someone else - this can be as simple as sharing a photo on social media or sending an update to a friend.\nBy incorporating these principles into your daily life, you\u0026rsquo;ll start to see changes in how you approach tasks, relationships, and even yourself. Remember that it\u0026rsquo;s okay to make mistakes along the way - failure is just another form of \u0026ldquo;done\u0026rdquo;!\n","date":"22 May 2024","externalUrl":null,"permalink":"/posts/done-manifesto/","section":"Posts","summary":"In today\u0026rsquo;s fast-paced world, getting things done is crucial. The Done manifesto is all about embracing this mindset and applying it to your work.","title":"Done Manifesto","type":"posts"},{"content":"","date":"22 May 2024","externalUrl":null,"permalink":"/tags/manifesto/","section":"Tags","summary":"","title":"Manifesto","type":"tags"},{"content":"","date":"20 May 2024","externalUrl":null,"permalink":"/tags/algorithm/","section":"Tags","summary":"","title":"Algorithm","type":"tags"},{"content":"","date":"20 May 2024","externalUrl":null,"permalink":"/tags/image/","section":"Tags","summary":"","title":"Image","type":"tags"},{"content":"","date":"20 May 2024","externalUrl":null,"permalink":"/tags/image-processing/","section":"Tags","summary":"","title":"Image Processing","type":"tags"},{"content":"","date":"20 May 2024","externalUrl":null,"permalink":"/tags/paper/","section":"Tags","summary":"","title":"Paper","type":"tags"},{"content":"","date":"20 May 2024","externalUrl":null,"permalink":"/tags/stipple/","section":"Tags","summary":"","title":"Stipple","type":"tags"},{"content":" The Art of Stippling # Stippling, a timeless artistic technique, traces its origins back through the annals of art history, where it emerged as a method of creating texture, depth, and form through the precise placement of dots. Dating back to ancient times, stippling found its earliest expressions in the intricate engravings of coins and the meticulous illustrations adorning manuscripts.\nFrom Pen to Pixel: Evolution of Stippling # Throughout the centuries, stippling evolved alongside advancements in artistic tools and techniques. Renaissance masters such as Albrecht Dürer and Leonardo da Vinci wielded the quill with virtuosic precision, employing stippling to imbue their works with a sense of realism and dimensionality. As the art world transitioned into the modern era, artists like Georges Seurat pioneered the technique of pointillism, using small, distinct dots of color to create vibrant, luminous compositions.\nThe Digital Renaissance: Stippling in the 21st Century # In the digital age, stippling underwent a renaissance of its own, propelled by the advent of computational algorithms and computer graphics. Artists and technologists embraced stippling as a means of blending traditional craftsmanship with cutting-edge technology, ushering in a new era of creative possibility.\nWeighted Voronoi Stippling: A Modern Marvel # At the forefront of this digital renaissance stands Weighted Voronoi Stippling, a groundbreaking technique that harnesses the power of computational algorithms to automate and enhance the stippling process. By incorporating weighted Voronoi diagrams, this method empowers artists to exert precise control over the distribution and density of stippled marks, breathing new life into the age-old practice of stippling.\nHonoring Tradition, Embracing Innovation # As we reflect on the rich history of stippling, we recognize its enduring appeal as a testament to the ingenuity and creativity of artists across the ages. From the humble beginnings of ink and parchment to the boundless realms of pixels and algorithms, stippling continues to captivate and inspire, bridging the gap between tradition and innovation in the ever-evolving tapestry of artistic expression.\nCreating Stippled Images with Weighted Voronoi Stippling: A Step-by-Step Guide # Weighted Voronoi Stippling is a powerful technique used in computer graphics and computational art to create stippled images that capture the essence of an input image. In this guide, we\u0026rsquo;ll walk through the implementation of this technique, providing step-by-step instructions along with pseudo-code to help you get started on your own projects.\nStep 1: Understand the Concept # Before diving into the implementation, it\u0026rsquo;s essential to understand the concept behind Weighted Voronoi Stippling. At its core, this technique involves distributing a set of points (stipples) across a canvas in a way that approximates an input image. The distribution is based on Voronoi diagrams, with the addition of weights to control the density of stipples in different regions of the image.\nStep 2: Gather Your Tools # To implement Weighted Voronoi Stippling, you\u0026rsquo;ll need basic knowledge of programming and computer graphics. You can use any programming language of your choice, but for the sake of this guide, we\u0026rsquo;ll provide pseudo-code examples that are easy to understand and can be translated into any language.\nStep 3: Generate Initial Stipples # The first step in the implementation process is to generate an initial set of stipples. These stipples will serve as the starting point for the iterative algorithm used to refine their positions.\nfunction generateStipples(numStipples):\rstipples = []\rfor i from 1 to numStipples:\rx = randomXCoordinate()\ry = randomYCoordinate()\rweight = calculateWeight(x, y) // Optional: Calculate weight based on input image\rstipples.append((x, y, weight))\rreturn stipples Step 4: Refine Stipple Positions # Next, we\u0026rsquo;ll iteratively refine the positions of the stipples based on their weighted contributions to the input image.\nfunction refineStipples(stipples, numIterations):\rfor i from 1 to numIterations:\rfor each stipple in stipples:\rx, y = stipple.position\rxNew, yNew = findNewPosition(x, y) // Use Lloyd\u0026#39;s relaxation or other techniques\rstipple.position = (xNew, yNew)\rreturn stipples Step 5: Render the Stippled Image # Once the stipple positions have been refined, it\u0026rsquo;s time to render the final stippled image. This can be done by rendering the Voronoi diagram formed by the stipples and applying shading based on the weights of the stipples.\nfunction renderStippledImage(stipples, canvas):\rfor each pixel in canvas:\rnearestStipple = findNearestStipple(pixel.position, stipples)\rpixel.color = nearestStipple.color // Optional: Use weighted shading for smoother results\rreturn canvas Step 6: Experiment and Fine-Tune # Weighted Voronoi Stippling offers a wide range of creative possibilities, so don\u0026rsquo;t hesitate to experiment with different parameters and techniques to achieve the desired effect. Fine-tune the number of stipples, the weighting function, and the rendering process to achieve the best results for your specific application.\nConclusion # By following this step-by-step guide and using the provided pseudo-code examples, you can implement Weighted Voronoi Stippling to create stunning stippled images that capture the essence of any input image.\n","date":"20 May 2024","externalUrl":null,"permalink":"/posts/weigthed-voronoi-stippling/","section":"Posts","summary":"Stippling, a timeless artistic technique, traces its origins back through the annals of art history, where it emerged as a method of creating texture, depth, and form through the precise placement of dots.","title":"Weighted Voronoi Stippling","type":"posts"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/data-analysis/","section":"Tags","summary":"","title":"Data Analysis","type":"tags"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/hidden/","section":"Tags","summary":"","title":"Hidden","type":"tags"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/hidden-markov-models/","section":"Tags","summary":"","title":"Hidden Markov Models","type":"tags"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/pattern/","section":"Tags","summary":"","title":"Pattern","type":"tags"},{"content":"","date":"19 May 2024","externalUrl":null,"permalink":"/tags/statistics/","section":"Tags","summary":"","title":"Statistics","type":"tags"},{"content":" A Voyage through Hidden Markov Models # In the realm of probabilistic modeling, few tools are as versatile and powerful as Hidden Markov Models (HMMs). From speech recognition to medical imaging, HMMs have left an indelible mark on a myriad of fields, shaping the way we understand and analyze sequential data. Join me on a voyage as we unravel the history, theory, key components, variations, and practical applications of Hidden Markov Models.\nA Glimpse into History: # The roots of HMMs trace back to the pioneering work of mathematician Andrey Markov in the late 19th century, who laid the groundwork for understanding stochastic processes. It wasn\u0026rsquo;t until the mid-20th century that researchers began to explore the extension of Markov processes to include hidden states. Key figures such as L. E. Baum and T. Petrie introduced seminal concepts, but it was their 1970 paper, \u0026ldquo;A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains,\u0026rdquo; that catalyzed the modern theory of HMMs. This groundbreaking paper introduced the forward-backward algorithm and the expectation-maximization (EM) algorithm, revolutionizing the field of probabilistic modeling.\nEssential Reading: # No exploration of HMMs would be complete without delving into Lawrence R. Rabiner\u0026rsquo;s timeless tutorial, \u0026ldquo;A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition,\u0026rdquo; published in the Proceedings of the IEEE in 1989. Rabiner\u0026rsquo;s comprehensive guide serves as a beacon for newcomers and seasoned researchers alike, offering deep insights into the principles, mathematics, and practical applications of HMMs, particularly in the realm of speech recognition.\nKey Components and Variations: # At the heart of Hidden Markov Models lie several key components:\nStates: Representing the hidden variables or underlying processes. Observations: Observable events influenced by the hidden states. Transition Probabilities: Likelihood of transitioning between hidden states. Emission Probabilities: Likelihood of observing specific events given the hidden states. HMMs also come in various forms and variations, including:\nContinuous HMMs: Where observations are continuous rather than discrete. Hidden semi-Markov models (HSMMs): Allowing for more complex state durations. Parameter Estimation Techniques: Such as the Baum-Welch algorithm for training HMMs from data. The Superpower of HMMs: # To wield the power of Hidden Markov Models is akin to possessing a superpower in the realm of data analysis. With the ability to uncover hidden patterns and relationships within sequential data, HMMs empower researchers and practitioners to extract actionable insights from complex datasets. Whether unraveling the mysteries of human speech, deciphering the secrets hidden within medical images, or forecasting financial trends, HMMs serve as indispensable tools for those seeking to unlock the full potential of their data.\nPractical Applications: # While the theoretical underpinnings of HMMs are fascinating, their true power shines through in their practical applications. Take, for example, the work of David H. Laidlaw et al., whose 1998 paper, \u0026ldquo;Application of Hidden Markov Models to Detecting White Matter Brain Lesions in Multiple Sclerosis Using Multichannel MRI,\u0026rdquo; showcases the transformative impact of HMMs in medical imaging. By leveraging the spatial and temporal characteristics of brain lesions as hidden states within an HMM framework, the authors achieved remarkable accuracy in detecting and segmenting lesions in MRI scans of patients with multiple sclerosis, opening new avenues for diagnosis and treatment.\n","date":"19 May 2024","externalUrl":null,"permalink":"/posts/hidden-markov-models/","section":"Posts","summary":"Hidden Markov Models (HMMs) are statistical models used for sequential data analysis, where underlying states are inferred from observed data. Employed in speech recognition, bioinformatics, and more.","title":"Voyage through Hidden Markov Models","type":"posts"},{"content":"","date":"15 May 2024","externalUrl":null,"permalink":"/tags/diagram/","section":"Tags","summary":"","title":"Diagram","type":"tags"},{"content":"","date":"15 May 2024","externalUrl":null,"permalink":"/tags/explanation/","section":"Tags","summary":"","title":"Explanation","type":"tags"},{"content":" Voronoi Diagram Explanation # Voronoi diagrams, also known as Dirichlet tessellation or Thiessen polygons, are everywhere in nature. You’ve likely encountered them thousands of times, but perhaps didn’t know what they were called. Voronoi diagrams are simple, yet they have incredible properties that have applications in fields ranging from cartography, biology, computer science, statistics, archaeology, all the way to architecture and arts.\nFirst, it should be noted that for any positive integer n, there are n-dimensional Voronoi diagrams, but for now we will only be dealing with two-dimensional Voronoi diagrams. The Voronoi diagram of a set of “sites” or “generators” (points) is a collection of regions that divide up the plane. Each region corresponds to one of the sites or generators, and all of the points in one region are closer to the corresponding site than to any other site. Where there is not one closest point, there is a boundary.\nAs an analogy imagine a Voronoi diagram in R^2 to contain a series of islands(our generator points). Suppose that each of these islands has a boat, with each boat capable of going the same speed. Let every point in R that can be reached from the boat from island x before any other boat can be associated with island x. The region of points associated with island x is called a Voronoi Diagram.\nThe basic idea of Voronoi Diagram has many applications in fields both within and outside the math world. Voronoi Diagrams can be used both within and outside the math world. Voronoi diagrams can be used as both a method of solving problems or as a model for examples that already exist. They are very useful in Computational Geometry, particularly for representation or quantization problems, and are used in the field of robotics for creating a protocol for avoiding detected obstacles. For modeling natural occurences, they are helpful in the studies of plant competition(echology \u0026amp; forestry), territories of animals(zoology) and neolithic clans and tribes(anthropology and archaelogy), and patterns of urban settelments(geography).\nVoronoi Diagram Definition # Suppose you have n points scattered on a plane, the Voronoi diagram of those points subdivides the plane in exactly n cells enclosing the portion of the plane that is the closest to each point. This produces a tessellation that completely covers the plane. In the illustration below, I plotted 100 random points and their corresponding Voronoi diagram. As you can see, every point is enclosed in a cell, whose boundaries are equidistant between two or more points. In other words, the area enclosed in the cell is closer to the point in the cell than to any other point.\nVoronoi Diagram\u0026rsquo;s History # Voronoi diagrams were considered as early at 1644 by René Descartes and were used by Dirichlet (1850) in the investigation of positive quadratic forms. They were also studied by Voronoi (1907), who extended the investigation of Voronoi diagrams to higher dimensions. They find widespread applications in areas such as computer graphics, epidemiology, geophysics, and meteorology. A particularly notable use of a Voronoi diagram was the analysis of the 1854 cholera epidemic in London, in which physician John Snow determined a strong correlation of deaths with proximity to a particular (and infected) water pump on Broad Street (Snow 1854, Snow 1855). In his analysis, Snow constructed a map on which he drew a line labeled \u0026ldquo;Boundary of equal distance between Broad Street Pump and other Pumps.\u0026rdquo; This line essentially indicated the Broad Street Pump\u0026rsquo;s Voronoi cell (Austin 2006). However, for an analysis highlighting some of the oversimplifications and misattributions in this folklore history account of the events surrounding Snow and the London cholera incident, see Field (2020).\nIn Nature # Voronoi diagram patterns are common in nature. From microscopic cells in onion skins, to the shell of jackfruits and the coat of giraffes, these patterns are everywhere.\nA reason for their omnipresence is that they form efficient shapes. As we mentioned earlier, a Voronoi diagram completely tessellates the plane. All space is used. This is very convenient if you are trying to squeeze as much as possible in a limited space — such as in muscle fibers or bee hives. Voronoi diagrams are also a spontaneous pattern whenever something is growing at a uniform growth rate from separate points as in the illustration below. For instance, this explains why giraffes exhibit such a pattern. Giraffe embryos have a scattered distribution of melanin-secreting cells, which is responsible for the dark pigmentation of the giraffe’s spots. Over the course of the gestation these cells release melanin — hence spots radiate outward. A study from researchers Marcelo Walter, Alan Fournier and Menevaux also explores this concept of using Voronoi diagrams to model computer rendering of spots on animal coats.\nIn architecture \u0026amp; art # Perhaps because of their spontaneous, natural look, or simply because of their mesmerizing randomness, Voronoi patterns have intentionally been implemented in human-made structures. An architectural example is the “Water cube,” which was built to house water sports during the 2008 Beijing Olympics. It features Voronoi diagrams on its ceiling and façades. The Voronoi diagrams were chosen because they recall bubbles . This analogy is clear at night, when the entire façade is illuminated in blue and comes alive.\nBut appreciation for the Voronoi pattern is surely older than this building in China. Guan and Ge ware from the Song dynasty have a distinctive crackled glaze. Ceramics can easily crack during the cooling process, however the crackles from the Guan and Ge ware are different because they are intentional. They were sought after because of their aesthetic qualities. Thanks to the Voronoi-like patterns on their surface, each piece is unique. To date, they are one of the most imitated styles of porcelain.\nVoronoi diagrams are also common in graphic arts for creating “abstract” patterns. I think they make excellent background images. For example, I created the thumbnail of this post by generating random points and constructing a Voronoi diagram. Then, I coloured each cell based on the distance of its point from a randomly selected spot in the box. Endless abstract backgrounds images could be generated this way.\nVoronoi Diagram \u0026amp; Delaunay Triangulation # The Delaunay triangulation and Voronoi diagram in R^2 are dual to each other in the graph theoretical sense.\n","date":"15 May 2024","externalUrl":null,"permalink":"/posts/voronoi-diagram/","section":"Posts","summary":"Voronoi diagrams, also known as Dirichlet tessellation or Thiessen polygons, are everywhere in nature. You’ve likely encountered them thousands of times, but perhaps didn’t know what they were called.","title":"Voronoi Diagram","type":"posts"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/classification/","section":"Tags","summary":"","title":"Classification","type":"tags"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/gzip/","section":"Tags","summary":"","title":"Gzip","type":"tags"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/kolmogorov/","section":"Tags","summary":"","title":"Kolmogorov","type":"tags"},{"content":" A review of \u0026ldquo;Less is More: Parameter-Free Text Classification with Gzip\u0026rdquo; # Introduction: # Text classification is a fundamental task in NLP, with applications ranging from sentiment analysis to spam detection. Traditional methods often require meticulous parameter tuning, which can be laborious and time-consuming. However, the authors of \u0026ldquo;Less is More\u0026rdquo; present a refreshing departure from this norm by harnessing the power of the gzip algorithm for feature extraction, thereby eliminating the need for manual parameter adjustments.\nUnderstanding the Approach: # At the heart of this paper lies a simple yet ingenious idea: leveraging gzip, a ubiquitous compression algorithm, to automatically derive features from textual data. By treating text as compressed data and exploiting gzip\u0026rsquo;s ability to capture redundancies and patterns, the proposed approach obviates the reliance on handcrafted parameters. Instead, it allows the algorithm to adapt organically to the inherent structure of the text, resulting in a parameter-free classification framework.\nKolmogorov Complexity and Compression: # The brilliance of using compression algorithms like gzip in text classification lies in their approximation of Kolmogorov complexity. Kolmogorov complexity refers to the minimum length of a computer program needed to generate a particular piece of data. While it\u0026rsquo;s a powerful theoretical concept, it\u0026rsquo;s practically impossible to implement directly due to its undecidability. However, compression algorithms like gzip offer a practical approximation of this complexity by identifying and exploiting patterns and redundancies in the data.\nKey Findings and Results: # Through a series of experiments conducted on various benchmark datasets, the authors demonstrate the efficacy of their approach. Notably, \u0026ldquo;Less is More\u0026rdquo; achieves competitive classification performance across different tasks while significantly reducing the computational overhead associated with parameter tuning. This streamlined approach not only simplifies the text classification pipeline but also enhances scalability and reproducibility.\nImplications and Future Directions: # The implications of this research extend beyond text classification, offering insights into the broader landscape of machine learning and data compression. By harnessing existing algorithms for novel purposes, we unlock new avenues for innovation and efficiency. Moreover, the parameter-free nature of the proposed method paves the way for seamless integration into real-world applications, where resource constraints and computational efficiency are paramount.\nConclusion: # In conclusion, \u0026ldquo;Less is More: Parameter-Free Text Classification with Gzip\u0026rdquo; represents a paradigm shift in the realm of text classification. By embracing simplicity and harnessing the power of compression algorithms, the authors have devised a robust and efficient framework that transcends conventional approaches. As we venture forward, this research serves as a beacon illuminating the path towards more streamlined and scalable NLP solutions.\nAs we reflect on the insights gleaned from \u0026ldquo;Less is More,\u0026rdquo; it becomes evident that simplicity and innovation are not mutually exclusive. Rather, they converge to usher in a new era of efficiency and effectiveness in text classification and beyond.\nlink to less is more\n","date":"5 May 2024","externalUrl":null,"permalink":"/posts/less-is-more/","section":"Posts","summary":"Less is More: Parameter-Free Text Classification with Gzip offers a novel text classification method using gzip compression, eliminating manual parameter tuning.","title":"Less is More Paper Review","type":"posts"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/nlp/","section":"Tags","summary":"","title":"NLP","type":"tags"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/text/","section":"Tags","summary":"","title":"Text","type":"tags"},{"content":" Difference of Gaussians Algorithm(DoG) # In the realm of image processing, where art meets science, techniques like the Difference of Gaussians (DoG) stand as pillars, providing us with tools to accentuate details, sharpen edges, and enhance visual clarity. In this comprehensive guide, we embark on an aesthetic journey to unravel the inner workings of the Difference of Gaussians, exploring its foundations, extensions, and applications.\nDoG Parameters # The Difference of Gaussians (DoG) algorithm involves several parameters that influence its operation and output. Here\u0026rsquo;s a comprehensive list of these parameters:\nStandard Deviation (σ): This parameter determines the spread or blurriness of the Gaussian filter. In DoG, two Gaussian filters are utilized, each with its own standard deviation.\nScalar: The scalar is a multiplier applied to the standard deviation of one of the Gaussian filters. It allows for the adjustment of the difference between the two Gaussian-blurred images, thus influencing the strength of the edge lines in the output.\nThreshold: After applying the Difference of Gaussians, a threshold can be applied to the output. This threshold determines which pixel values are considered edges and which are not, by specifying a cutoff value. Pixels with values above the threshold are typically set to white, while those below are set to black.\nSigma C: In the extended version of DoG( xDoG), introduced by Winnemoeller, Sigma C represents the standard deviation of the structure tensor after Gaussian blurring. It influences the blurring of the structure tensor, affecting the style and sharpness of the rendered edges.\nSigma E: Another parameter introduced in Winnemoeller\u0026rsquo;s extension, Sigma E dictates the standard deviation of the one-dimensional blur across edges. It determines how much the Gaussian blur is applied along the edges, contributing to the overall appearance of the output.\nSigma M: In the Line Integral Convolution (LIC) stage, Sigma M represents the standard deviation of the Gaussian blur applied along the edge lines. It influences the degree of blurring along these lines, smoothing out the output and reducing noise.\nSigma A: A parameter introduced for anti-aliasing in the second Line Integral Convolution (LIC) step. Sigma A represents the standard deviation of the Gaussian blur applied to smooth out jagged edges and improve the visual quality of the output.\nUnderstanding and fine-tuning these parameters is crucial for optimizing the performance and achieving desired results with the Difference of Gaussians algorithm.\nUnderstanding the Basics # At its core, Difference of Gaussians operates on the principle of subtracting one Gaussian-blurred image from another. Here\u0026rsquo;s the essence distilled: take a Gaussian filter with a certain standard deviation, subtract another Gaussian filter with a different standard deviation multiplied by a scalar. What you get are accentuated edge lines. But how does this seemingly simple operation achieve such remarkable results?\nThe Low-Pass Filter # To comprehend the magic behind DoG, we delve into the realm of signal processing. The Gaussian function, a quintessential tool in the signal processor\u0026rsquo;s arsenal, acts as a low-pass filter. In simple terms, it suppresses high frequencies while preserving lower frequencies. By applying two Gaussian filters with varying deviations and subtracting them, we create a band-pass filter that selectively allows through frequencies associated with high contrast areas-often synonymous with edges.\nThe Evolution: Winnemoeller\u0026rsquo;s Contribution # While Difference of Gaussians laid a solid foundation, Winnemoeller\u0026rsquo;s work addressed a critical dilemma: the balance between sharpness and noise. Enter the Extended Difference of Gaussians. By borrowing insights from the Anisotropic Kuwahara filter, Winnemoeller introduced the concept of Edge Tangent Flow. This flow, derived from convolving the image with the Sobel operator to approximate partial derivatives, paved the way for a more nuanced approach.\nSigma C and Sigma E: The Building Blocks # Here\u0026rsquo;s where the plot thickens. We introduce two new parameters: Sigma C and Sigma E. Sigma C represents the standard deviation of the structure tensor after Gaussian blurring, while Sigma E dictates the standard deviation of the one-dimensional blur across edges. These parameters play a pivotal role in shaping the final output, offering control over the style and sharpness of the rendered edges.\nLine Integral Convolution: Blurring Along Edge Lines # Ever wondered how to blur along edge lines? Line Integral Convolution (LIC) holds the answer. Leveraging the edge tangent flow-a vector field where vectors point in the direction of edge lines-LIC smoothens the output by blurring along these lines. By sampling pixels and corresponding vectors, applying Gaussian blurs, and traversing along the flow field, LIC emerges as a powerful technique for visualizing flow fields and enhancing image clarity.\nAnti-Aliasing with Sigma A # As we gaze upon our thresholded Difference of Gaussians, we notice aliasing rearing its head. But fear not, for Sigma A comes to the rescue. By applying a second Line Integral Convolution with a standard deviation represented by Sigma A, we smooth out those jagged edges, elevating the visual appeal and fidelity of our output.\nConclusion \u0026amp; Practical Applications # In conclusion, Difference of Gaussians stands as a testament to the fusion of art and science in the realm of image processing. From its humble beginnings as a subtraction operation to its evolution into a sophisticated algorithm with extended capabilities, DoG continues to shape the way we perceive and enhance visual imagery. Difference of Gaussians is commonly used in computer vision, image processing, and feature detection tasks due to its effectiveness in highlighting edges and features while suppressing noise. It is a foundational technique in many edge detection algorithms and serves as a building block for more advanced image processing methods.\n","date":"4 May 2024","externalUrl":null,"permalink":"/posts/difference-of-gaussians/","section":"Posts","summary":"The Difference of Gaussians (DoG) algorithm is a technique in image processing used for edge detection and feature enhancement.","title":"Difference of Gaussians(DoG) Algorithm","type":"posts"},{"content":"","date":"4 May 2024","externalUrl":null,"permalink":"/tags/edge-detection/","section":"Tags","summary":"","title":"Edge Detection","type":"tags"},{"content":"","date":"4 May 2024","externalUrl":null,"permalink":"/tags/gaussian/","section":"Tags","summary":"","title":"Gaussian","type":"tags"},{"content":"","date":"3 May 2024","externalUrl":null,"permalink":"/tags/attention/","section":"Tags","summary":"","title":"Attention","type":"tags"},{"content":"","date":"3 May 2024","externalUrl":null,"permalink":"/tags/google/","section":"Tags","summary":"","title":"Google","type":"tags"},{"content":" Unveiling Infini-Attention # In the ever-evolving landscape of natural language processing, scaling Transformer-based language models (LLMs) to accommodate infinitely long inputs while constraining memory and computation has long been a tantalizing goal. Recently, a groundbreaking paper has emerged, promising to fulfill this vision: Infini-Attention. Let\u0026rsquo;s delve into the intricacies of this innovative approach and understand how it aims to reshape the future of LLMs.\nThe Challenge of Scale # Traditional attention mechanisms, while powerful, encounter limitations when confronted with extensive inputs. The quadratic nature of softmax-based attention restricts the scalability of Transformer models, capping out at a mere 1000 parameters. Linear algebra offers a potential solution, yet early attempts fell short on complex tasks, highlighting the need for a more sophisticated approach.\nEnter Infini-Attention # Infini-Attention introduces a paradigm shift by integrating compressive memory within the vanilla attention mechanism of Transformers. This novel approach combines masked local attention and long-term linear attention mechanisms within a single transformer block, enabling efficient handling of extensive inputs with minimal memory parameters.\nDual Mechanism # Similar to TransformerXL, Infini-Attention divides its attention mechanism into two parts: traditional multi-head attention and a novel compressive memory and linear attention module. These components work synergistically, augmenting the primary signal with information from the compressive memory, which accumulates relevant past data.\nMethodology and Equations # The methodology behind Infini-Attention revolves around building and retrieving from compressive memory. Leveraging a learned gating scalar, termed Beta, the model seamlessly integrates information from both current and past contexts. The formulae for memory retrieval and update, though complex, underscore the model\u0026rsquo;s sophistication in managing information flow.\nUnveiling the Magic # The essence of Infini-Attention lies in its ability to leverage current queries to access a compressed representation of past key-value combinations. By employing a clever non-linearity (sigmoid), the model approximates the functionality of softmax, optimizing memory utilization without redundancy. This approach mirrors a recurrent neural network\u0026rsquo;s behavior, albeit without its inherent drawbacks.\nConclusion: Beyond the Horizon # Infini-Attention emerges as a beacon of innovation in the realm of Transformer-based LLMs. By seamlessly blending traditional attention mechanisms with compressive memory and linear attention, it paves the way for handling infinitely long inputs with finesse. While linearized attention mechanisms of the past faltered, Infini-Attention stands poised to deliver on its promise, ushering in a new era of limitless language processing capabilities.\nIn summary, Infini-Attention not only promises to overcome the constraints of traditional attention mechanisms but also sets the stage for transformative advancements in natural language understanding. With its blend of ingenuity and sophistication, it represents a significant leap forward in the quest for scalable and efficient language models.\nLink to Infini-Attention\n","date":"3 May 2024","externalUrl":null,"permalink":"/posts/infini-attention/","section":"Posts","summary":"Infini-Attention introduces a novel approach to scaling Transformer models for infinitely long inputs while managing memory and computation.","title":"Infini-Attention Paper Review","type":"posts"},{"content":"","date":"3 May 2024","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"3 May 2024","externalUrl":null,"permalink":"/tags/transformer/","section":"Tags","summary":"","title":"Transformer","type":"tags"},{"content":"","date":"19 April 2024","externalUrl":null,"permalink":"/tags/softmax/","section":"Tags","summary":"","title":"Softmax","type":"tags"},{"content":" Introduction # Over the past decade, several groundbreaking Neural Network models have emerged, reshaping the landscape of artificial intelligence and machine learning. Here\u0026rsquo;s a curated list of the most impactful models released during this period:\nAlexNet (2012):\nContribution: This model pioneered the application of deep convolutional neural networks (CNNs) in image classification tasks, demonstrating the potential of deep learning in large-scale visual recognition. Influence: Its success ignited widespread interest in deep learning research and laid the foundation for subsequent advancements in CNN architectures. GoogleNet (Inception) (2014):\nContribution: GoogleNet introduced inception modules to enhance computational efficiency in deep neural networks. It also popularized techniques like global average pooling and auxiliary classifiers. Influence: Its innovative architecture inspired the development of more efficient models and spurred research into model compactness and computational efficiency. VGGNet (2014):\nContribution: VGGNet emphasized the significance of network depth by employing a straightforward yet deep architecture composed of repeated 3x3 convolutional layers. Influence: Its depth-focused design motivated further exploration of deeper networks and influenced subsequent architectures aiming for improved performance through increased depth. Seq2Seq Models (2014):\nContribution: Seq2Seq models introduced the encoder-decoder architecture, enabling tasks such as machine translation, text summarization, and speech recognition. Influence: They revolutionized sequence modeling tasks and paved the way for attention mechanisms in neural networks. ResNet (2015):\nContribution: ResNet addressed the challenge of training very deep neural networks by introducing residual connections, which alleviated the vanishing gradient problem. Influence: It led to the development of extremely deep architectures and became a staple in state-of-the-art models. DenseNet (2016):\nContribution: DenseNet introduced dense connectivity patterns between layers, promoting feature reuse and facilitating gradient flow in deep neural networks. Influence: Its architecture inspired models prioritizing feature reuse and gradient flow, resulting in improvements in parameter efficiency and performance. Transformer (2017):\nContribution: The Transformer model revolutionized natural language processing (NLP) with its self-attention mechanism, enabling effective modeling of long-range dependencies in sequences. Influence: It catalyzed the development of transformer-based models that achieved state-of-the-art performance across various NLP tasks. BERT (2018):\nContribution: BERT introduced pre-training of contextualized word embeddings using large-scale unlabeled text corpora, enabling transfer learning for downstream NLP tasks. Influence: It spurred research in transfer learning and contextualized embeddings, leading to the creation of diverse pre-trained language models with numerous applications. EfficientNet (2019):\nContribution: EfficientNet proposed a scalable and efficient CNN architecture that achieved state-of-the-art performance across different resource constraints by balancing network depth, width, and resolution. Influence: It highlighted the importance of model scaling for efficient and effective neural network design, inspiring research into scalable architectures. GPT-2 (2019):\nContribution: GPT-2 introduced a large-scale transformer-based language model capable of generating coherent and contextually relevant text on a wide range of topics. Influence: It expanded the boundaries of language generation and showcased the capabilities of large-scale transformer models for natural language understanding and generation tasks. These models represent significant milestones in neural network research, each contributing unique advancements that have shaped the field and laid the groundwork for further innovation. Their interconnectedness underscores the iterative nature of deep learning research, where each advancement builds upon existing models to push the boundaries of what is possible.\nRole of Softmax in Model Architectures # While not all models explicitly use the softmax function, many rely on it as a vital component for tasks like classification, probability estimation, and sequence generation. Let\u0026rsquo;s explore how some of these models leverage and benefit from the softmax function:\nAlexNet:\nAlexNet typically employs softmax activation in its final layer to convert raw output scores into class probabilities for image classification tasks. After passing through convolutional and pooling layers, features are flattened and fed into a fully connected layer followed by softmax, yielding a probability distribution over classes. GoogleNet (Inception):\nAlthough GoogleNet (Inception) doesn\u0026rsquo;t directly utilize softmax in its inception modules, it often incorporates softmax in the final layer for classification. Inception modules generate feature maps, which are aggregated, processed, and then passed through a softmax layer to obtain class probabilities. VGGNet:\nSimilar to AlexNet, VGGNet typically employs softmax activation in its final layer for image classification. After multiple convolutional and pooling layers, flattened features are passed through fully connected layers followed by softmax to produce class probabilities. Seq2Seq Models:\nIn tasks like machine translation or text summarization, Seq2Seq models often employ softmax activation in the decoder to generate probability distributions over the vocabulary at each time step. Softmax is applied to output logits to obtain probabilities, aiding in selecting the most probable token. BERT:\nWhile BERT doesn\u0026rsquo;t use softmax during pre-training, it often utilizes softmax for fine-tuning on downstream tasks like text classification or named entity recognition. BERT\u0026rsquo;s output representations pass through a softmax layer to obtain probabilities over different classes or labels in these tasks. GPT-2:\nGPT-2 uses softmax activation in its output layer for text generation. At each time step, the model predicts the next token by applying softmax to logits produced by the final layer, generating a probability distribution over the vocabulary. In all cases, the softmax function plays a pivotal role in converting raw model outputs into interpretable probability distributions, facilitating tasks like classification, sequence generation, and language modeling. Additionally, softmax activations produce gradients crucial for training via backpropagation and stochastic gradient descent, making it integral to the optimization process.\nSoftmax Function and Its Relationship with Cross-Entropy Loss # Understanding the relationship between the softmax function and the cross-entropy loss function is crucial for classification tasks in neural networks. Let\u0026rsquo;s delve into this relationship using mathematical notation:\nSoftmax Function:\nThe softmax function transforms a vector of real numbers into a probability distribution, commonly used in the output layer of neural networks for multi-class classification. It\u0026rsquo;s defined as: [ \\text{softmax}(\\mathbf{z})i = \\frac{e^{z_i}}{\\sum{j=1}^{K} e^{z_j}} ]\nWhere:\n( \\mathbf{z} = [z_1, z_2, \u0026hellip;, z_K] ) is the input vector of raw output scores (logits). ( K ) is the number of classes. ( e ) represents Euler\u0026rsquo;s number (approximately 2.71828). ( \\text{softmax}(\\mathbf{z})_i ) denotes the probability of the ( i )-th class after applying softmax. Cross-Entropy Loss Function:\nThe cross-entropy loss measures the dissimilarity between the predicted probability distribution (obtained from softmax) and the true label distribution. For multi-class classification, it\u0026rsquo;s defined as: [ \\text{Cross-Entropy Loss} = - \\sum_{i=1}^{K} y_i \\log(\\hat{y}_i) ] Where: ( K ) is the number of classes. ( y_i ) is the true probability of the ( i )-th class (either 0 or 1). ( \\hat{y}_i ) is the predicted probability of the ( i )-th class obtained from softmax output. Relationship:\nThe softmax function computes predicted probabilities of each class, while the cross-entropy loss evaluates how closely these predicted probabilities match the true labels. During training, minimizing cross-entropy loss encourages the model to produce predicted probabilities aligning with the true label distribution, facilitating accurate predictions in classification tasks. Softmax Function Definition # The softmax function is a mathematical operation commonly used in machine learning and statistics to convert a vector of real numbers into a probability distribution. Its formula is:\n[ \\text{softmax}(\\mathbf{z})i = \\frac{e^{z_i}}{\\sum{j=1}^{K} e^{z_j}} ]\nWhere:\n( \\mathbf{z} = [z_1, z_2, \u0026hellip;, z_K] ) is the input vector. ( K ) denotes the number of elements in the vector. ( e ) represents Euler\u0026rsquo;s number (approximately 2.71828). ( \\text{softmax}(\\mathbf{z})_i ) represents the ( i )-th element of the output vector after applying softmax. The softmax function exponentiates each element of the input vector and normalizes these values by dividing them by the sum of all exponentials, ensuring the output vector sums to 1, thus forming a valid probability distribution.\nMathematical Properties of Softmax # The softmax function possesses several mathematical properties, making it a valuable tool in machine learning for multi-class classification tasks. These properties include:\nOutput as Probability Distribution:\nSoftmax transforms input into a probability distribution, with each element representing the probability of the corresponding class, facilitating interpretability. Normalization:\nIt normalizes input values to ensure output probabilities are well-defined and independent of input scale. Monotonicity:\nSoftmax is a monotonic transformation, ensuring increasing input values lead to higher corresponding output probabilities. Sensitivity to Input Differences:\nSoftmax amplifies differences between input values, with higher input values yielding higher output probabilities. Differentiability:\nSoftmax is differentiable everywhere, enabling efficient computation of gradients for optimization. Numerical Stability:\nSoftmax is designed to handle numerical instability associated with exponentiating large or small input values, aiding in numerical robustness during computation. These properties collectively make softmax a fundamental component in classification tasks, providing a means to convert raw scores into probabilities efficiently.\nWidespread Use of Softmax # Softmax enjoys widespread adoption due to several factors:\nOutput Interpretation: Softmax ensures neural network outputs represent probabilities, facilitating easy interpretation where each element denotes the probability of input belonging to a class.\nGradient-friendly: Softmax\u0026rsquo;s differentiability enables efficient computation of gradients, crucial for training neural networks using algorithms like stochastic gradient descent.\nNumerical Stability: Softmax handles numerical instability associated with exponentiation, mitigating issues like overflow or underflow.\nCompatibility with Cross-Entropy Loss: Softmax naturally pairs with cross-entropy loss in many classification tasks, simplifying optimization and promoting convergence during training.\nProbabilistic Representation: Softmax naturally represents model outputs as probability distributions, making it suitable for tasks requiring probabilistic interpretations like classification.\nAlternatives to Softmax # Several alternatives to softmax exist, each with unique advantages and disadvantages, catering to specific task requirements:\nSigmoid Function: Suitable for binary classification tasks but requires modifications for multi-class classification.\nLogistic Function: Extensible to multi-class classification through one-vs-all approach but may suffer from vanishing gradients.\nArcTan Function: Smooth and continuous but less commonly used for classification tasks.\nGaussian Function: Suitable for tasks with Gaussian output distributions but computationally expensive.\nSoftplus Function: Efficiently avoids vanishing gradients but outputs are not normalized.\nSparsemax Function: Encourages sparsity in output probabilities but requires careful hyperparameter tuning.\nMaxout Function: Generalizes ReLU for complex functions but is computationally expensive and prone to overfitting.\nThe choice of activation function depends on task requirements, data nature, and computational considerations, with softmax remaining a popular choice for its simplicity, interpretability, and compatibility with classification tasks.\nSummary # Softmax stands as a pivotal component in neural network architectures, offering a means to convert raw scores into interpretable probabilities. Its widespread use is attributed to its compatibility with training algorithms, numerical stability, and natural integration with loss functions. Understanding softmax and its properties is essential for effectively leveraging it in classification tasks, contributing to the advancement of machine learning and artificial intelligence.\n","date":"19 April 2024","externalUrl":null,"permalink":"/posts/softmax/","section":"Posts","summary":"Softmax stands as a pivotal component in neural network architectures, offering a means to convert raw scores into interpretable probabilities.","title":"Softmax","type":"posts"},{"content":"","date":"17 April 2024","externalUrl":null,"permalink":"/tags/dither/","section":"Tags","summary":"","title":"Dither","type":"tags"},{"content":" Introduction # In the realm of digital image processing, dithering algorithms play a crucial role in reducing the color palette of an image while maintaining visual quality. These algorithms distribute quantization errors across neighboring pixels, resulting in visually pleasing images with fewer colors. In this blog post, we\u0026rsquo;ll delve into the implementation of two popular dithering algorithms, Floyd-Steinberg and Atkinson, using the power of Numba for performance optimization.\nUnderstanding Dithering Algorithms # Before we delve into the code, let\u0026rsquo;s briefly understand the two dithering algorithms we\u0026rsquo;ll be exploring:\nFloyd-Steinberg Dithering: Developed by Robert W. Floyd and Louis Steinberg in 1976. Distributes quantization errors to neighboring pixels in a specific pattern. Produces sharp images with noticeable noise. Atkinson Dithering: Developed by Bill Atkinson in 1982. Similar to Floyd-Steinberg but distributes errors differently. Produces smoother images with less visible noise. Implementation with Numba # Now, let\u0026rsquo;s see how we can implement these dithering algorithms efficiently using Numba, a Just-In-Time compiler for Python code.\n@numba.jit(\u0026#34;f4[:,:,:](f4[:,:,:])\u0026#34;, nopython=True, nogil=True) def floyd_steinberg(image): Lx, Ly, Lc = image.shape for j in range(Ly): for i in range(Lx): for c in range(Lc): rounded = round(image[i,j,c]) err = image[i,j,c] - rounded image[i,j,c] = rounded if i\u0026lt;Lx-1: image[i+1,j,c] += (7/16)*err if j\u0026lt;Ly-1: image[i,j+1,c] += (5/16)*err if i\u0026gt;0: image[i-1,j+1,c] += (1/16)*err if i\u0026lt;Lx-1: image[i+1,j+1,c] += (3/16)*err return image @numba.jit(\u0026#34;f4[:,:,:](f4[:,:,:])\u0026#34;, nopython=True, nogil=True) def atkinson(image): frac = 8 Lx, Ly, Lc = image.shape for j in range(Ly): for i in range(Lx): for c in range(Lc): rounded = round(image[i,j,c]) err = image[i,j,c] - rounded image[i,j,c] = rounded if i\u0026lt;Lx-1: image[i+1,j,c] += err / frac if i\u0026lt;Lx-2: image[i+2,j,c] += err /frac if j\u0026lt;Ly-1: image[i,j+1,c] += err / frac if i\u0026gt;0: image[i-1,j+1,c] += err / frac if i\u0026lt;Lx-1: image[i+1,j+1,c] += err / frac if j\u0026lt;Ly-2: image[i,j+2,c] += err / frac return image Explanation of the Code # We utilize NumPy for numerical operations, PIL (Python Imaging Library) for image loading and saving, and Numba for JIT compilation to enhance performance. Both Floyd-Steinberg and Atkinson algorithms are implemented as Numba-jitted functions. The algorithms iterate through each pixel of the image, applying error diffusion to distribute quantization errors. Finally, the processed images are saved to disk. Results \u0026amp; Conclusion # By applying Floyd-Steinberg and Atkinson dithering algorithms to an input image, we\u0026rsquo;ve successfully reduced its color palette while preserving visual quality. The utilization of Numba for performance optimization ensures efficient processing, making these algorithms suitable for large-scale image manipulation tasks.\nExperimentation with different images and tweaking parameters can yield varying results, allowing for customization based on specific requirements. Dithering algorithms continue to be relevant in various applications, including digital art, printing, and image compression.\nIn conclusion, by exploring dithering algorithms such as Floyd-Steinberg and Atkinson and leveraging the power of Numba for implementation, we\u0026rsquo;ve gained insights into enhancing image processing tasks with efficient and optimized code.\nLink to Complete Implementation in GitHub\nAestheticVoyager/dither-filter Python 0 0 ","date":"17 April 2024","externalUrl":null,"permalink":"/posts/dither/","section":"Posts","summary":"In the realm of digital image processing, dithering algorithms play a crucial role in reducing the color palette of an image while maintaining visual quality.","title":"Dither","type":"posts"},{"content":"","date":"17 April 2024","externalUrl":null,"permalink":"/tags/filter/","section":"Tags","summary":"","title":"Filter","type":"tags"},{"content":"","date":"17 April 2024","externalUrl":null,"permalink":"/tags/fitler/","section":"Tags","summary":"","title":"Fitler","type":"tags"},{"content":"","date":"17 April 2024","externalUrl":null,"permalink":"/tags/kuwahara/","section":"Tags","summary":"","title":"Kuwahara","type":"tags"},{"content":"The Kuwahara filter is a non-linear smoothing filter used in image processing for adaptive noise reduction. Most filters that are used for image smoothing are linear low-pass filters that effectively reduce noise but also blur out the edges. However the Kuwahara filter is able to apply smoothing on the image while preserving the edges.\nIt is named after Michiyoshi Kuwahara, Ph.D., who worked at Kyoto and Osaka Sangyo Universities in Japan, developing early medical imaging of dynamic heart muscle in the 1970s and 80s.\nKuwahara Filter description # The Kuwahara filter works on a window divided into 4 overlapping sub-windows. In each sub-window, the mean and variance are computed.\nThe output value (located at the center of the window) is set to the mean of the sub-window with the smallest variance.\nApplications # Originally the Kuwahara filter was proposed for use in processing RI-angiocardiographic images of the cardiovascular system.\nThe fact that any edges are preserved when smoothing makes it especially useful for feature extraction and segmentation and explains why it is used in medical imaging.\nThe Kuwahara filter however also finds many applications in artistic imaging and fine-art photography due to its ability to remove textures and sharpen the edges of photographs. The level of abstraction helps create a desirable painting-like effect in artistic photographs especially in the case of the colored image version of the filter. These applications have known great success and have encouraged similar research in the field of image processing for the arts.\nAlthough the vast majority of applications have been in the field of image processing there have been cases that use modifications of the Kuwahara filter for machine learning tasks such as clustering.\nThe Kuwahara filter has been implemented in CVIPtools.\nAnisotropic Kuwahara Filtering with Polynomial Weighting Functions Paper # The Anisotropic Kuwahara Paper link\nKuwahara was the world\u0026rsquo;s first edge preserving de-noising image processing algorithm. It was upgraded by the \u0026ldquo;Anisotropic Kuwahara Filtering with Polynomial Weighting Functions\u0026rdquo; paper, by:\nUpgraded by using a circular kernel instead of Box kernel. Instead of using naive weights, we use gaussian weights. This new formula: 1/(1+std_div), sector color = Ki, K(x)=(sum of Ki * Wi)/(sum of weights i) This removes indeterminate behavior and removes all conditional logic of the old algorithm. All these changes were made by Guiseppe Papari.\nThankfully we can just ditch the Gauss and instead approximate the weight using \u0026ldquo;Polynomials\u0026rdquo;.\nThen we\u0026rsquo;ll calculate the Eigen-Values. To calculate the Eigen-Values of the structure tensor and use them to calculate the eigenvectors that points in the direction of the minimum rate of change. We\u0026rsquo;re just essentially figuring out what direction a pixel points in using the eigenvector information.\nThe filter kernel can now angle itself and stretch itself to better fit image details and edges.\nThis new filter is called Anisotropic Kuwahara Filter.\nRecommendation: In-order to achieve High Contrast Visuals, it is better to apply the anisotropic kuwahara then apply the dither effect.\nMy Personal Optimized Implementation of Kuwahara filter # Personal Implementation AestheticVoyager/kuwahara-filter Python 0 0 ","date":"17 April 2024","externalUrl":null,"permalink":"/posts/kuwahara/","section":"Posts","summary":"Kuwahara was the world\u0026rsquo;s first edge preserving de-noising image processing algorithm.","title":"Kuwahara","type":"posts"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]