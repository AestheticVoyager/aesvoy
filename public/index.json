
[{"content":"","date":"15 May 2024","externalUrl":null,"permalink":"/","section":"AesVoy","summary":"","title":"AesVoy","type":"page"},{"content":"","date":"15 May 2024","externalUrl":null,"permalink":"/tags/algorithm/","section":"Tags","summary":"","title":"Algorithm","type":"tags"},{"content":"","date":"15 May 2024","externalUrl":null,"permalink":"/tags/computational-geometry/","section":"Tags","summary":"","title":"Computational Geometry","type":"tags"},{"content":"","date":"15 May 2024","externalUrl":null,"permalink":"/tags/diagram/","section":"Tags","summary":"","title":"Diagram","type":"tags"},{"content":"","date":"15 May 2024","externalUrl":null,"permalink":"/tags/explanation/","section":"Tags","summary":"","title":"Explanation","type":"tags"},{"content":"","date":"15 May 2024","externalUrl":null,"permalink":"/tags/image/","section":"Tags","summary":"","title":"Image","type":"tags"},{"content":"","date":"15 May 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"15 May 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"15 May 2024","externalUrl":null,"permalink":"/tags/voronoi/","section":"Tags","summary":"","title":"Voronoi","type":"tags"},{"content":" Voronoi Diagram Explanation # Voronoi diagrams, also known as Dirichlet tessellation or Thiessen polygons, are everywhere in nature. You’ve likely encountered them thousands of times, but perhaps didn’t know what they were called. Voronoi diagrams are simple, yet they have incredible properties that have applications in fields ranging from cartography, biology, computer science, statistics, archaeology, all the way to architecture and arts.\nFirst, it should be noted that for any positive integer n, there are n-dimensional Voronoi diagrams, but for now we will only be dealing with two-dimensional Voronoi diagrams. The Voronoi diagram of a set of “sites” or “generators” (points) is a collection of regions that divide up the plane. Each region corresponds to one of the sites or generators, and all of the points in one region are closer to the corresponding site than to any other site. Where there is not one closest point, there is a boundary.\nAs an analogy imagine a Voronoi diagram in R^2 to contain a series of islands(our generator points). Suppose that each of these islands has a boat, with each boat capable of going the same speed. Let every point in R that can be reached from the boat from island x before any other boat can be associated with island x. The region of points associated with island x is called a Voronoi Diagram.\nThe basic idea of Voronoi Diagram has many applications in fields both within and outside the math world. Voronoi Diagrams can be used both within and outside the math world. Voronoi diagrams can be used as both a method of solving problems or as a model for examples that already exist. They are very useful in Computational Geometry, particularly for representation or quantization problems, and are used in the field of robotics for creating a protocol for avoiding detected obstacles. For modeling natural occurences, they are helpful in the studies of plant competition(echology \u0026amp; forestry), territories of animals(zoology) and neolithic clans and tribes(anthropology and archaelogy), and patterns of urban settelments(geography).\nVoronoi Diagram Definition # Suppose you have n points scattered on a plane, the Voronoi diagram of those points subdivides the plane in exactly n cells enclosing the portion of the plane that is the closest to each point. This produces a tessellation that completely covers the plane. In the illustration below, I plotted 100 random points and their corresponding Voronoi diagram. As you can see, every point is enclosed in a cell, whose boundaries are equidistant between two or more points. In other words, the area enclosed in the cell is closer to the point in the cell than to any other point.\nVoronoi Diagram\u0026rsquo;s History # Voronoi diagrams were considered as early at 1644 by René Descartes and were used by Dirichlet (1850) in the investigation of positive quadratic forms. They were also studied by Voronoi (1907), who extended the investigation of Voronoi diagrams to higher dimensions. They find widespread applications in areas such as computer graphics, epidemiology, geophysics, and meteorology. A particularly notable use of a Voronoi diagram was the analysis of the 1854 cholera epidemic in London, in which physician John Snow determined a strong correlation of deaths with proximity to a particular (and infected) water pump on Broad Street (Snow 1854, Snow 1855). In his analysis, Snow constructed a map on which he drew a line labeled \u0026ldquo;Boundary of equal distance between Broad Street Pump and other Pumps.\u0026rdquo; This line essentially indicated the Broad Street Pump\u0026rsquo;s Voronoi cell (Austin 2006). However, for an analysis highlighting some of the oversimplifications and misattributions in this folklore history account of the events surrounding Snow and the London cholera incident, see Field (2020).\nIn Nature # Voronoi diagram patterns are common in nature. From microscopic cells in onion skins, to the shell of jackfruits and the coat of giraffes, these patterns are everywhere.\nA reason for their omnipresence is that they form efficient shapes. As we mentioned earlier, a Voronoi diagram completely tessellates the plane. All space is used. This is very convenient if you are trying to squeeze as much as possible in a limited space — such as in muscle fibers or bee hives. Voronoi diagrams are also a spontaneous pattern whenever something is growing at a uniform growth rate from separate points as in the illustration below. For instance, this explains why giraffes exhibit such a pattern. Giraffe embryos have a scattered distribution of melanin-secreting cells, which is responsible for the dark pigmentation of the giraffe’s spots. Over the course of the gestation these cells release melanin — hence spots radiate outward. A study from researchers Marcelo Walter, Alan Fournier and Menevaux also explores this concept of using Voronoi diagrams to model computer rendering of spots on animal coats.\nIn architecture \u0026amp; art # Perhaps because of their spontaneous, natural look, or simply because of their mesmerizing randomness, Voronoi patterns have intentionally been implemented in human-made structures. An architectural example is the “Water cube,” which was built to house water sports during the 2008 Beijing Olympics. It features Voronoi diagrams on its ceiling and façades. The Voronoi diagrams were chosen because they recall bubbles . This analogy is clear at night, when the entire façade is illuminated in blue and comes alive.\nBut appreciation for the Voronoi pattern is surely older than this building in China. Guan and Ge ware from the Song dynasty have a distinctive crackled glaze. Ceramics can easily crack during the cooling process, however the crackles from the Guan and Ge ware are different because they are intentional. They were sought after because of their aesthetic qualities. Thanks to the Voronoi-like patterns on their surface, each piece is unique. To date, they are one of the most imitated styles of porcelain.\nVoronoi diagrams are also common in graphic arts for creating “abstract” patterns. I think they make excellent background images. For example, I created the thumbnail of this post by generating random points and constructing a Voronoi diagram. Then, I coloured each cell based on the distance of its point from a randomly selected spot in the box. Endless abstract backgrounds images could be generated this way.\nVoronoi Diagram \u0026amp; Delaunay Triangulation # The Delaunay triangulation and Voronoi diagram in R^2 are dual to each other in the graph theoretical sense.\n","date":"15 May 2024","externalUrl":null,"permalink":"/posts/voronoi-diagram/","section":"Posts","summary":"Voronoi Diagram Explanation # Voronoi diagrams, also known as Dirichlet tessellation or Thiessen polygons, are everywhere in nature.","title":"Voronoi Diagram","type":"posts"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/classification/","section":"Tags","summary":"","title":"Classification","type":"tags"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/gzip/","section":"Tags","summary":"","title":"Gzip","type":"tags"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/kolmogorov/","section":"Tags","summary":"","title":"Kolmogorov","type":"tags"},{"content":" A review of \u0026ldquo;Less is More: Parameter-Free Text Classification with Gzip\u0026rdquo; # Introduction: # Text classification is a fundamental task in NLP, with applications ranging from sentiment analysis to spam detection. Traditional methods often require meticulous parameter tuning, which can be laborious and time-consuming. However, the authors of \u0026ldquo;Less is More\u0026rdquo; present a refreshing departure from this norm by harnessing the power of the gzip algorithm for feature extraction, thereby eliminating the need for manual parameter adjustments.\nUnderstanding the Approach: # At the heart of this paper lies a simple yet ingenious idea: leveraging gzip, a ubiquitous compression algorithm, to automatically derive features from textual data. By treating text as compressed data and exploiting gzip\u0026rsquo;s ability to capture redundancies and patterns, the proposed approach obviates the reliance on handcrafted parameters. Instead, it allows the algorithm to adapt organically to the inherent structure of the text, resulting in a parameter-free classification framework.\nKolmogorov Complexity and Compression: # The brilliance of using compression algorithms like gzip in text classification lies in their approximation of Kolmogorov complexity. Kolmogorov complexity refers to the minimum length of a computer program needed to generate a particular piece of data. While it\u0026rsquo;s a powerful theoretical concept, it\u0026rsquo;s practically impossible to implement directly due to its undecidability. However, compression algorithms like gzip offer a practical approximation of this complexity by identifying and exploiting patterns and redundancies in the data.\nKey Findings and Results: # Through a series of experiments conducted on various benchmark datasets, the authors demonstrate the efficacy of their approach. Notably, \u0026ldquo;Less is More\u0026rdquo; achieves competitive classification performance across different tasks while significantly reducing the computational overhead associated with parameter tuning. This streamlined approach not only simplifies the text classification pipeline but also enhances scalability and reproducibility.\nImplications and Future Directions: # The implications of this research extend beyond text classification, offering insights into the broader landscape of machine learning and data compression. By harnessing existing algorithms for novel purposes, we unlock new avenues for innovation and efficiency. Moreover, the parameter-free nature of the proposed method paves the way for seamless integration into real-world applications, where resource constraints and computational efficiency are paramount.\nConclusion: # In conclusion, \u0026ldquo;Less is More: Parameter-Free Text Classification with Gzip\u0026rdquo; represents a paradigm shift in the realm of text classification. By embracing simplicity and harnessing the power of compression algorithms, the authors have devised a robust and efficient framework that transcends conventional approaches. As we venture forward, this research serves as a beacon illuminating the path towards more streamlined and scalable NLP solutions.\nAs we reflect on the insights gleaned from \u0026ldquo;Less is More,\u0026rdquo; it becomes evident that simplicity and innovation are not mutually exclusive. Rather, they converge to usher in a new era of efficiency and effectiveness in text classification and beyond.\nlink to less is more\n","date":"5 May 2024","externalUrl":null,"permalink":"/posts/less-is-more/","section":"Posts","summary":"Less is More: Parameter-Free Text Classification with Gzip offers a novel text classification method using gzip compression, eliminating manual parameter tuning.","title":"Less is More Paper Review","type":"posts"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/ml/","section":"Tags","summary":"","title":"ML","type":"tags"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/nlp/","section":"Tags","summary":"","title":"NLP","type":"tags"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/paper/","section":"Tags","summary":"","title":"Paper","type":"tags"},{"content":"","date":"5 May 2024","externalUrl":null,"permalink":"/tags/text/","section":"Tags","summary":"","title":"Text","type":"tags"},{"content":" Difference of Gaussians Algorithm(DoG) # In the realm of image processing, where art meets science, techniques like the Difference of Gaussians (DoG) stand as pillars, providing us with tools to accentuate details, sharpen edges, and enhance visual clarity. In this comprehensive guide, we embark on an aesthetic journey to unravel the inner workings of the Difference of Gaussians, exploring its foundations, extensions, and applications.\nDoG Parameters # The Difference of Gaussians (DoG) algorithm involves several parameters that influence its operation and output. Here\u0026rsquo;s a comprehensive list of these parameters:\nStandard Deviation (σ): This parameter determines the spread or blurriness of the Gaussian filter. In DoG, two Gaussian filters are utilized, each with its own standard deviation.\nScalar: The scalar is a multiplier applied to the standard deviation of one of the Gaussian filters. It allows for the adjustment of the difference between the two Gaussian-blurred images, thus influencing the strength of the edge lines in the output.\nThreshold: After applying the Difference of Gaussians, a threshold can be applied to the output. This threshold determines which pixel values are considered edges and which are not, by specifying a cutoff value. Pixels with values above the threshold are typically set to white, while those below are set to black.\nSigma C: In the extended version of DoG( xDoG), introduced by Winnemoeller, Sigma C represents the standard deviation of the structure tensor after Gaussian blurring. It influences the blurring of the structure tensor, affecting the style and sharpness of the rendered edges.\nSigma E: Another parameter introduced in Winnemoeller\u0026rsquo;s extension, Sigma E dictates the standard deviation of the one-dimensional blur across edges. It determines how much the Gaussian blur is applied along the edges, contributing to the overall appearance of the output.\nSigma M: In the Line Integral Convolution (LIC) stage, Sigma M represents the standard deviation of the Gaussian blur applied along the edge lines. It influences the degree of blurring along these lines, smoothing out the output and reducing noise.\nSigma A: A parameter introduced for anti-aliasing in the second Line Integral Convolution (LIC) step. Sigma A represents the standard deviation of the Gaussian blur applied to smooth out jagged edges and improve the visual quality of the output.\nUnderstanding and fine-tuning these parameters is crucial for optimizing the performance and achieving desired results with the Difference of Gaussians algorithm.\nUnderstanding the Basics # At its core, Difference of Gaussians operates on the principle of subtracting one Gaussian-blurred image from another. Here\u0026rsquo;s the essence distilled: take a Gaussian filter with a certain standard deviation, subtract another Gaussian filter with a different standard deviation multiplied by a scalar. What you get are accentuated edge lines. But how does this seemingly simple operation achieve such remarkable results?\nThe Low-Pass Filter # To comprehend the magic behind DoG, we delve into the realm of signal processing. The Gaussian function, a quintessential tool in the signal processor\u0026rsquo;s arsenal, acts as a low-pass filter. In simple terms, it suppresses high frequencies while preserving lower frequencies. By applying two Gaussian filters with varying deviations and subtracting them, we create a band-pass filter that selectively allows through frequencies associated with high contrast areas-often synonymous with edges.\nThe Evolution: Winnemoeller\u0026rsquo;s Contribution # While Difference of Gaussians laid a solid foundation, Winnemoeller\u0026rsquo;s work addressed a critical dilemma: the balance between sharpness and noise. Enter the Extended Difference of Gaussians. By borrowing insights from the Anisotropic Kuwahara filter, Winnemoeller introduced the concept of Edge Tangent Flow. This flow, derived from convolving the image with the Sobel operator to approximate partial derivatives, paved the way for a more nuanced approach.\nSigma C and Sigma E: The Building Blocks # Here\u0026rsquo;s where the plot thickens. We introduce two new parameters: Sigma C and Sigma E. Sigma C represents the standard deviation of the structure tensor after Gaussian blurring, while Sigma E dictates the standard deviation of the one-dimensional blur across edges. These parameters play a pivotal role in shaping the final output, offering control over the style and sharpness of the rendered edges.\nLine Integral Convolution: Blurring Along Edge Lines # Ever wondered how to blur along edge lines? Line Integral Convolution (LIC) holds the answer. Leveraging the edge tangent flow-a vector field where vectors point in the direction of edge lines-LIC smoothens the output by blurring along these lines. By sampling pixels and corresponding vectors, applying Gaussian blurs, and traversing along the flow field, LIC emerges as a powerful technique for visualizing flow fields and enhancing image clarity.\nAnti-Aliasing with Sigma A # As we gaze upon our thresholded Difference of Gaussians, we notice aliasing rearing its head. But fear not, for Sigma A comes to the rescue. By applying a second Line Integral Convolution with a standard deviation represented by Sigma A, we smooth out those jagged edges, elevating the visual appeal and fidelity of our output.\nConclusion \u0026amp; Practical Applications # In conclusion, Difference of Gaussians stands as a testament to the fusion of art and science in the realm of image processing. From its humble beginnings as a subtraction operation to its evolution into a sophisticated algorithm with extended capabilities, DoG continues to shape the way we perceive and enhance visual imagery. Difference of Gaussians is commonly used in computer vision, image processing, and feature detection tasks due to its effectiveness in highlighting edges and features while suppressing noise. It is a foundational technique in many edge detection algorithms and serves as a building block for more advanced image processing methods.\n","date":"4 May 2024","externalUrl":null,"permalink":"/posts/difference-of-gaussians/","section":"Posts","summary":"The Difference of Gaussians (DoG) algorithm is a technique in image processing used for edge detection and feature enhancement.","title":"Difference of Gaussians(DoG) Algorithm","type":"posts"},{"content":"","date":"4 May 2024","externalUrl":null,"permalink":"/tags/edge-detection/","section":"Tags","summary":"","title":"Edge Detection","type":"tags"},{"content":"","date":"4 May 2024","externalUrl":null,"permalink":"/tags/gaussian/","section":"Tags","summary":"","title":"Gaussian","type":"tags"},{"content":"","date":"4 May 2024","externalUrl":null,"permalink":"/tags/image-processing/","section":"Tags","summary":"","title":"Image Processing","type":"tags"},{"content":"","date":"3 May 2024","externalUrl":null,"permalink":"/tags/attention/","section":"Tags","summary":"","title":"Attention","type":"tags"},{"content":"","date":"3 May 2024","externalUrl":null,"permalink":"/tags/google/","section":"Tags","summary":"","title":"Google","type":"tags"},{"content":" Unveiling Infini-Attention # In the ever-evolving landscape of natural language processing, scaling Transformer-based language models (LLMs) to accommodate infinitely long inputs while constraining memory and computation has long been a tantalizing goal. Recently, a groundbreaking paper has emerged, promising to fulfill this vision: Infini-Attention. Let\u0026rsquo;s delve into the intricacies of this innovative approach and understand how it aims to reshape the future of LLMs.\nThe Challenge of Scale # Traditional attention mechanisms, while powerful, encounter limitations when confronted with extensive inputs. The quadratic nature of softmax-based attention restricts the scalability of Transformer models, capping out at a mere 1000 parameters. Linear algebra offers a potential solution, yet early attempts fell short on complex tasks, highlighting the need for a more sophisticated approach.\nEnter Infini-Attention # Infini-Attention introduces a paradigm shift by integrating compressive memory within the vanilla attention mechanism of Transformers. This novel approach combines masked local attention and long-term linear attention mechanisms within a single transformer block, enabling efficient handling of extensive inputs with minimal memory parameters.\nDual Mechanism # Similar to TransformerXL, Infini-Attention divides its attention mechanism into two parts: traditional multi-head attention and a novel compressive memory and linear attention module. These components work synergistically, augmenting the primary signal with information from the compressive memory, which accumulates relevant past data.\nMethodology and Equations # The methodology behind Infini-Attention revolves around building and retrieving from compressive memory. Leveraging a learned gating scalar, termed Beta, the model seamlessly integrates information from both current and past contexts. The formulae for memory retrieval and update, though complex, underscore the model\u0026rsquo;s sophistication in managing information flow.\nUnveiling the Magic # The essence of Infini-Attention lies in its ability to leverage current queries to access a compressed representation of past key-value combinations. By employing a clever non-linearity (sigmoid), the model approximates the functionality of softmax, optimizing memory utilization without redundancy. This approach mirrors a recurrent neural network\u0026rsquo;s behavior, albeit without its inherent drawbacks.\nConclusion: Beyond the Horizon # Infini-Attention emerges as a beacon of innovation in the realm of Transformer-based LLMs. By seamlessly blending traditional attention mechanisms with compressive memory and linear attention, it paves the way for handling infinitely long inputs with finesse. While linearized attention mechanisms of the past faltered, Infini-Attention stands poised to deliver on its promise, ushering in a new era of limitless language processing capabilities.\nIn summary, Infini-Attention not only promises to overcome the constraints of traditional attention mechanisms but also sets the stage for transformative advancements in natural language understanding. With its blend of ingenuity and sophistication, it represents a significant leap forward in the quest for scalable and efficient language models.\nLink to Infini-Attention\n","date":"3 May 2024","externalUrl":null,"permalink":"/posts/infini-attention/","section":"Posts","summary":"Infini-Attention introduces a novel approach to scaling Transformer models for infinitely long inputs while managing memory and computation.","title":"Infini-Attention Paper Review","type":"posts"},{"content":"","date":"3 May 2024","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"3 May 2024","externalUrl":null,"permalink":"/tags/transformer/","section":"Tags","summary":"","title":"Transformer","type":"tags"},{"content":"","date":"19 April 2024","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"19 April 2024","externalUrl":null,"permalink":"/tags/softmax/","section":"Tags","summary":"","title":"Softmax","type":"tags"},{"content":" Introduction # Over the past decade, several groundbreaking Neural Network models have emerged, reshaping the landscape of artificial intelligence and machine learning. Here\u0026rsquo;s a curated list of the most impactful models released during this period:\nAlexNet (2012):\nContribution: This model pioneered the application of deep convolutional neural networks (CNNs) in image classification tasks, demonstrating the potential of deep learning in large-scale visual recognition. Influence: Its success ignited widespread interest in deep learning research and laid the foundation for subsequent advancements in CNN architectures. GoogleNet (Inception) (2014):\nContribution: GoogleNet introduced inception modules to enhance computational efficiency in deep neural networks. It also popularized techniques like global average pooling and auxiliary classifiers. Influence: Its innovative architecture inspired the development of more efficient models and spurred research into model compactness and computational efficiency. VGGNet (2014):\nContribution: VGGNet emphasized the significance of network depth by employing a straightforward yet deep architecture composed of repeated 3x3 convolutional layers. Influence: Its depth-focused design motivated further exploration of deeper networks and influenced subsequent architectures aiming for improved performance through increased depth. Seq2Seq Models (2014):\nContribution: Seq2Seq models introduced the encoder-decoder architecture, enabling tasks such as machine translation, text summarization, and speech recognition. Influence: They revolutionized sequence modeling tasks and paved the way for attention mechanisms in neural networks. ResNet (2015):\nContribution: ResNet addressed the challenge of training very deep neural networks by introducing residual connections, which alleviated the vanishing gradient problem. Influence: It led to the development of extremely deep architectures and became a staple in state-of-the-art models. DenseNet (2016):\nContribution: DenseNet introduced dense connectivity patterns between layers, promoting feature reuse and facilitating gradient flow in deep neural networks. Influence: Its architecture inspired models prioritizing feature reuse and gradient flow, resulting in improvements in parameter efficiency and performance. Transformer (2017):\nContribution: The Transformer model revolutionized natural language processing (NLP) with its self-attention mechanism, enabling effective modeling of long-range dependencies in sequences. Influence: It catalyzed the development of transformer-based models that achieved state-of-the-art performance across various NLP tasks. BERT (2018):\nContribution: BERT introduced pre-training of contextualized word embeddings using large-scale unlabeled text corpora, enabling transfer learning for downstream NLP tasks. Influence: It spurred research in transfer learning and contextualized embeddings, leading to the creation of diverse pre-trained language models with numerous applications. EfficientNet (2019):\nContribution: EfficientNet proposed a scalable and efficient CNN architecture that achieved state-of-the-art performance across different resource constraints by balancing network depth, width, and resolution. Influence: It highlighted the importance of model scaling for efficient and effective neural network design, inspiring research into scalable architectures. GPT-2 (2019):\nContribution: GPT-2 introduced a large-scale transformer-based language model capable of generating coherent and contextually relevant text on a wide range of topics. Influence: It expanded the boundaries of language generation and showcased the capabilities of large-scale transformer models for natural language understanding and generation tasks. These models represent significant milestones in neural network research, each contributing unique advancements that have shaped the field and laid the groundwork for further innovation. Their interconnectedness underscores the iterative nature of deep learning research, where each advancement builds upon existing models to push the boundaries of what is possible.\nRole of Softmax in Model Architectures # While not all models explicitly use the softmax function, many rely on it as a vital component for tasks like classification, probability estimation, and sequence generation. Let\u0026rsquo;s explore how some of these models leverage and benefit from the softmax function:\nAlexNet:\nAlexNet typically employs softmax activation in its final layer to convert raw output scores into class probabilities for image classification tasks. After passing through convolutional and pooling layers, features are flattened and fed into a fully connected layer followed by softmax, yielding a probability distribution over classes. GoogleNet (Inception):\nAlthough GoogleNet (Inception) doesn\u0026rsquo;t directly utilize softmax in its inception modules, it often incorporates softmax in the final layer for classification. Inception modules generate feature maps, which are aggregated, processed, and then passed through a softmax layer to obtain class probabilities. VGGNet:\nSimilar to AlexNet, VGGNet typically employs softmax activation in its final layer for image classification. After multiple convolutional and pooling layers, flattened features are passed through fully connected layers followed by softmax to produce class probabilities. Seq2Seq Models:\nIn tasks like machine translation or text summarization, Seq2Seq models often employ softmax activation in the decoder to generate probability distributions over the vocabulary at each time step. Softmax is applied to output logits to obtain probabilities, aiding in selecting the most probable token. BERT:\nWhile BERT doesn\u0026rsquo;t use softmax during pre-training, it often utilizes softmax for fine-tuning on downstream tasks like text classification or named entity recognition. BERT\u0026rsquo;s output representations pass through a softmax layer to obtain probabilities over different classes or labels in these tasks. GPT-2:\nGPT-2 uses softmax activation in its output layer for text generation. At each time step, the model predicts the next token by applying softmax to logits produced by the final layer, generating a probability distribution over the vocabulary. In all cases, the softmax function plays a pivotal role in converting raw model outputs into interpretable probability distributions, facilitating tasks like classification, sequence generation, and language modeling. Additionally, softmax activations produce gradients crucial for training via backpropagation and stochastic gradient descent, making it integral to the optimization process.\nSoftmax Function and Its Relationship with Cross-Entropy Loss # Understanding the relationship between the softmax function and the cross-entropy loss function is crucial for classification tasks in neural networks. Let\u0026rsquo;s delve into this relationship using mathematical notation:\nSoftmax Function:\nThe softmax function transforms a vector of real numbers into a probability distribution, commonly used in the output layer of neural networks for multi-class classification. It\u0026rsquo;s defined as: [ \\text{softmax}(\\mathbf{z})i = \\frac{e^{z_i}}{\\sum{j=1}^{K} e^{z_j}} ]\nWhere:\n( \\mathbf{z} = [z_1, z_2, \u0026hellip;, z_K] ) is the input vector of raw output scores (logits). ( K ) is the number of classes. ( e ) represents Euler\u0026rsquo;s number (approximately 2.71828). ( \\text{softmax}(\\mathbf{z})_i ) denotes the probability of the ( i )-th class after applying softmax. Cross-Entropy Loss Function:\nThe cross-entropy loss measures the dissimilarity between the predicted probability distribution (obtained from softmax) and the true label distribution. For multi-class classification, it\u0026rsquo;s defined as: [ \\text{Cross-Entropy Loss} = - \\sum_{i=1}^{K} y_i \\log(\\hat{y}_i) ] Where: ( K ) is the number of classes. ( y_i ) is the true probability of the ( i )-th class (either 0 or 1). ( \\hat{y}_i ) is the predicted probability of the ( i )-th class obtained from softmax output. Relationship:\nThe softmax function computes predicted probabilities of each class, while the cross-entropy loss evaluates how closely these predicted probabilities match the true labels. During training, minimizing cross-entropy loss encourages the model to produce predicted probabilities aligning with the true label distribution, facilitating accurate predictions in classification tasks. Softmax Function Definition # The softmax function is a mathematical operation commonly used in machine learning and statistics to convert a vector of real numbers into a probability distribution. Its formula is:\n[ \\text{softmax}(\\mathbf{z})i = \\frac{e^{z_i}}{\\sum{j=1}^{K} e^{z_j}} ]\nWhere:\n( \\mathbf{z} = [z_1, z_2, \u0026hellip;, z_K] ) is the input vector. ( K ) denotes the number of elements in the vector. ( e ) represents Euler\u0026rsquo;s number (approximately 2.71828). ( \\text{softmax}(\\mathbf{z})_i ) represents the ( i )-th element of the output vector after applying softmax. The softmax function exponentiates each element of the input vector and normalizes these values by dividing them by the sum of all exponentials, ensuring the output vector sums to 1, thus forming a valid probability distribution.\nMathematical Properties of Softmax # The softmax function possesses several mathematical properties, making it a valuable tool in machine learning for multi-class classification tasks. These properties include:\nOutput as Probability Distribution:\nSoftmax transforms input into a probability distribution, with each element representing the probability of the corresponding class, facilitating interpretability. Normalization:\nIt normalizes input values to ensure output probabilities are well-defined and independent of input scale. Monotonicity:\nSoftmax is a monotonic transformation, ensuring increasing input values lead to higher corresponding output probabilities. Sensitivity to Input Differences:\nSoftmax amplifies differences between input values, with higher input values yielding higher output probabilities. Differentiability:\nSoftmax is differentiable everywhere, enabling efficient computation of gradients for optimization. Numerical Stability:\nSoftmax is designed to handle numerical instability associated with exponentiating large or small input values, aiding in numerical robustness during computation. These properties collectively make softmax a fundamental component in classification tasks, providing a means to convert raw scores into probabilities efficiently.\nWidespread Use of Softmax # Softmax enjoys widespread adoption due to several factors:\nOutput Interpretation: Softmax ensures neural network outputs represent probabilities, facilitating easy interpretation where each element denotes the probability of input belonging to a class.\nGradient-friendly: Softmax\u0026rsquo;s differentiability enables efficient computation of gradients, crucial for training neural networks using algorithms like stochastic gradient descent.\nNumerical Stability: Softmax handles numerical instability associated with exponentiation, mitigating issues like overflow or underflow.\nCompatibility with Cross-Entropy Loss: Softmax naturally pairs with cross-entropy loss in many classification tasks, simplifying optimization and promoting convergence during training.\nProbabilistic Representation: Softmax naturally represents model outputs as probability distributions, making it suitable for tasks requiring probabilistic interpretations like classification.\nAlternatives to Softmax # Several alternatives to softmax exist, each with unique advantages and disadvantages, catering to specific task requirements:\nSigmoid Function: Suitable for binary classification tasks but requires modifications for multi-class classification.\nLogistic Function: Extensible to multi-class classification through one-vs-all approach but may suffer from vanishing gradients.\nArcTan Function: Smooth and continuous but less commonly used for classification tasks.\nGaussian Function: Suitable for tasks with Gaussian output distributions but computationally expensive.\nSoftplus Function: Efficiently avoids vanishing gradients but outputs are not normalized.\nSparsemax Function: Encourages sparsity in output probabilities but requires careful hyperparameter tuning.\nMaxout Function: Generalizes ReLU for complex functions but is computationally expensive and prone to overfitting.\nThe choice of activation function depends on task requirements, data nature, and computational considerations, with softmax remaining a popular choice for its simplicity, interpretability, and compatibility with classification tasks.\nSummary # Softmax stands as a pivotal component in neural network architectures, offering a means to convert raw scores into interpretable probabilities. Its widespread use is attributed to its compatibility with training algorithms, numerical stability, and natural integration with loss functions. Understanding softmax and its properties is essential for effectively leveraging it in classification tasks, contributing to the advancement of machine learning and artificial intelligence.\n","date":"19 April 2024","externalUrl":null,"permalink":"/posts/softmax/","section":"Posts","summary":"Softmax stands as a pivotal component in neural network architectures, offering a means to convert raw scores into interpretable probabilities.","title":"Softmax","type":"posts"},{"content":"","date":"17 April 2024","externalUrl":null,"permalink":"/tags/dither/","section":"Tags","summary":"","title":"Dither","type":"tags"},{"content":" Introduction # In the realm of digital image processing, dithering algorithms play a crucial role in reducing the color palette of an image while maintaining visual quality. These algorithms distribute quantization errors across neighboring pixels, resulting in visually pleasing images with fewer colors. In this blog post, we\u0026rsquo;ll delve into the implementation of two popular dithering algorithms, Floyd-Steinberg and Atkinson, using the power of Numba for performance optimization.\nUnderstanding Dithering Algorithms # Before we delve into the code, let\u0026rsquo;s briefly understand the two dithering algorithms we\u0026rsquo;ll be exploring:\nFloyd-Steinberg Dithering: Developed by Robert W. Floyd and Louis Steinberg in 1976. Distributes quantization errors to neighboring pixels in a specific pattern. Produces sharp images with noticeable noise. Atkinson Dithering: Developed by Bill Atkinson in 1982. Similar to Floyd-Steinberg but distributes errors differently. Produces smoother images with less visible noise. Implementation with Numba # Now, let\u0026rsquo;s see how we can implement these dithering algorithms efficiently using Numba, a Just-In-Time compiler for Python code.\n@numba.jit(\u0026#34;f4[:,:,:](f4[:,:,:])\u0026#34;, nopython=True, nogil=True) def floyd_steinberg(image): Lx, Ly, Lc = image.shape for j in range(Ly): for i in range(Lx): for c in range(Lc): rounded = round(image[i,j,c]) err = image[i,j,c] - rounded image[i,j,c] = rounded if i\u0026lt;Lx-1: image[i+1,j,c] += (7/16)*err if j\u0026lt;Ly-1: image[i,j+1,c] += (5/16)*err if i\u0026gt;0: image[i-1,j+1,c] += (1/16)*err if i\u0026lt;Lx-1: image[i+1,j+1,c] += (3/16)*err return image @numba.jit(\u0026#34;f4[:,:,:](f4[:,:,:])\u0026#34;, nopython=True, nogil=True) def atkinson(image): frac = 8 Lx, Ly, Lc = image.shape for j in range(Ly): for i in range(Lx): for c in range(Lc): rounded = round(image[i,j,c]) err = image[i,j,c] - rounded image[i,j,c] = rounded if i\u0026lt;Lx-1: image[i+1,j,c] += err / frac if i\u0026lt;Lx-2: image[i+2,j,c] += err /frac if j\u0026lt;Ly-1: image[i,j+1,c] += err / frac if i\u0026gt;0: image[i-1,j+1,c] += err / frac if i\u0026lt;Lx-1: image[i+1,j+1,c] += err / frac if j\u0026lt;Ly-2: image[i,j+2,c] += err / frac return image Explanation of the Code # We utilize NumPy for numerical operations, PIL (Python Imaging Library) for image loading and saving, and Numba for JIT compilation to enhance performance. Both Floyd-Steinberg and Atkinson algorithms are implemented as Numba-jitted functions. The algorithms iterate through each pixel of the image, applying error diffusion to distribute quantization errors. Finally, the processed images are saved to disk. Results \u0026amp; Conclusion # By applying Floyd-Steinberg and Atkinson dithering algorithms to an input image, we\u0026rsquo;ve successfully reduced its color palette while preserving visual quality. The utilization of Numba for performance optimization ensures efficient processing, making these algorithms suitable for large-scale image manipulation tasks.\nExperimentation with different images and tweaking parameters can yield varying results, allowing for customization based on specific requirements. Dithering algorithms continue to be relevant in various applications, including digital art, printing, and image compression.\nIn conclusion, by exploring dithering algorithms such as Floyd-Steinberg and Atkinson and leveraging the power of Numba for implementation, we\u0026rsquo;ve gained insights into enhancing image processing tasks with efficient and optimized code.\nLink to Complete Implementation in GitHub\nAestheticVoyager/dither-filter Python 0 0 ","date":"17 April 2024","externalUrl":null,"permalink":"/posts/dither/","section":"Posts","summary":"In the realm of digital image processing, dithering algorithms play a crucial role in reducing the color palette of an image while maintaining visual quality.","title":"Dither","type":"posts"},{"content":"","date":"17 April 2024","externalUrl":null,"permalink":"/tags/filter/","section":"Tags","summary":"","title":"Filter","type":"tags"},{"content":"","date":"17 April 2024","externalUrl":null,"permalink":"/tags/fitler/","section":"Tags","summary":"","title":"Fitler","type":"tags"},{"content":"","date":"17 April 2024","externalUrl":null,"permalink":"/tags/kuwahara/","section":"Tags","summary":"","title":"Kuwahara","type":"tags"},{"content":"The Kuwahara filter is a non-linear smoothing filter used in image processing for adaptive noise reduction. Most filters that are used for image smoothing are linear low-pass filters that effectively reduce noise but also blur out the edges. However the Kuwahara filter is able to apply smoothing on the image while preserving the edges.\nIt is named after Michiyoshi Kuwahara, Ph.D., who worked at Kyoto and Osaka Sangyo Universities in Japan, developing early medical imaging of dynamic heart muscle in the 1970s and 80s.\nKuwahara Filter description # The Kuwahara filter works on a window divided into 4 overlapping sub-windows. In each sub-window, the mean and variance are computed.\nThe output value (located at the center of the window) is set to the mean of the sub-window with the smallest variance.\nApplications # Originally the Kuwahara filter was proposed for use in processing RI-angiocardiographic images of the cardiovascular system.\nThe fact that any edges are preserved when smoothing makes it especially useful for feature extraction and segmentation and explains why it is used in medical imaging.\nThe Kuwahara filter however also finds many applications in artistic imaging and fine-art photography due to its ability to remove textures and sharpen the edges of photographs. The level of abstraction helps create a desirable painting-like effect in artistic photographs especially in the case of the colored image version of the filter. These applications have known great success and have encouraged similar research in the field of image processing for the arts.\nAlthough the vast majority of applications have been in the field of image processing there have been cases that use modifications of the Kuwahara filter for machine learning tasks such as clustering.\nThe Kuwahara filter has been implemented in CVIPtools.\nAnisotropic Kuwahara Filtering with Polynomial Weighting Functions Paper # The Anisotropic Kuwahara Paper link\nKuwahara was the world\u0026rsquo;s first edge preserving de-noising image processing algorithm. It was upgraded by the \u0026ldquo;Anisotropic Kuwahara Filtering with Polynomial Weighting Functions\u0026rdquo; paper, by:\nUpgraded by using a circular kernel instead of Box kernel. Instead of using naive weights, we use gaussian weights. This new formula: 1/(1+std_div), sector color = Ki, K(x)=(sum of Ki * Wi)/(sum of weights i) This removes indeterminate behavior and removes all conditional logic of the old algorithm. All these changes were made by Guiseppe Papari.\nThankfully we can just ditch the Gauss and instead approximate the weight using \u0026ldquo;Polynomials\u0026rdquo;.\nThen we\u0026rsquo;ll calculate the Eigen-Values. To calculate the Eigen-Values of the structure tensor and use them to calculate the eigenvectors that points in the direction of the minimum rate of change. We\u0026rsquo;re just essentially figuring out what direction a pixel points in using the eigenvector information.\nThe filter kernel can now angle itself and stretch itself to better fit image details and edges.\nThis new filter is called Anisotropic Kuwahara Filter.\nRecommendation: In-order to achieve High Contrast Visuals, it is better to apply the anisotropic kuwahara then apply the dither effect.\nMy Personal Optimized Implementation of Kuwahara filter # Personal Implementation AestheticVoyager/kuwahara-filter Python 0 0 ","date":"17 April 2024","externalUrl":null,"permalink":"/posts/kuwahara/","section":"Posts","summary":"Kuwahara was the world\u0026rsquo;s first edge preserving de-noising image processing algorithm.","title":"Kuwahara","type":"posts"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/tags/stipple/","section":"Tags","summary":"","title":"Stipple","type":"tags"}]