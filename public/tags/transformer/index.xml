<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformer on AesVoy</title>
    <link>https://aestheticvoyager.github.io/tags/transformer/</link>
    <description>Recent content in Transformer on AesVoy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Â© 2024 Mahan</copyright>
    <lastBuildDate>Thu, 08 Aug 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://aestheticvoyager.github.io/tags/transformer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>From CNNs to Vision Transformers: The Future of Image Recognition</title>
      <link>https://aestheticvoyager.github.io/posts/vit/</link>
      <pubDate>Thu, 08 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>https://aestheticvoyager.github.io/posts/vit/</guid>
      <description>Vision Transformers (ViTs) are redefining image recognition by using Transformer models to capture global context, unlike traditional Convolutional Neural Networks (CNNs) that focus on local features. ViTs excel with large datasets and show impressive scalability and performance.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://aestheticvoyager.github.io/posts/vit/featured.png" />
    </item>
    
    <item>
      <title>Transformers &amp; Attention</title>
      <link>https://aestheticvoyager.github.io/posts/attention-transformer/</link>
      <pubDate>Mon, 05 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>https://aestheticvoyager.github.io/posts/attention-transformer/</guid>
      <description>This blog post explains how self-attention and softmax function in Transformer models, crucial for modern NLP. It breaks down how self-attention helps models understand relationships between tokens and how softmax ensures efficient computation and numerical stability.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://aestheticvoyager.github.io/posts/attention-transformer/featured.jpg" />
    </item>
    
    <item>
      <title>Infini-Attention Paper Review</title>
      <link>https://aestheticvoyager.github.io/posts/infini-attention/</link>
      <pubDate>Fri, 03 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://aestheticvoyager.github.io/posts/infini-attention/</guid>
      <description>Infini-Attention introduces a novel approach to scaling Transformer models for infinitely long inputs while managing memory and computation.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://aestheticvoyager.github.io/posts/infini-attention/featured.png" />
    </item>
    
  </channel>
</rss>
