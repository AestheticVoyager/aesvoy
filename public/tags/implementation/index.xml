<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Implementation on Aesvoy</title>
    <link>https://aestheticvoyager.github.io/tags/implementation/</link>
    <description>Recent content in Implementation on Aesvoy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>sciredomir@tutanota.com (Mahan)</managingEditor>
    <webMaster>sciredomir@tutanota.com (Mahan)</webMaster>
    <copyright>© 2025 Mahan</copyright>
    <lastBuildDate>Sat, 06 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://aestheticvoyager.github.io/tags/implementation/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The Ackermann Function: Taming the Wildest Recursion in Computer Science</title>
      <link>https://aestheticvoyager.github.io/posts/ackermann/</link>
      <pubDate>Sat, 06 Sep 2025 00:00:00 +0000</pubDate>
      <author>sciredomir@tutanota.com (Mahan)</author>
      <guid>https://aestheticvoyager.github.io/posts/ackermann/</guid>
      <description>The Ackermann function is a deceptively simple algorithm that stands as a landmark in theoretical computer science. Defined by a concise set of recursive rules, it generates numerical values that grow at a rate faster than any primitive recursive function, quickly reaching magnitudes that are physically incomputable. While its naive implementation serves as a classic example of a recursion depth stress test, its true importance is historical and philosophical.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://aestheticvoyager.github.io/posts/ackermann/feature.png" />
    </item>
    
    <item>
      <title>ResNet Overview and Implementatoin</title>
      <link>https://aestheticvoyager.github.io/posts/resnet/</link>
      <pubDate>Thu, 04 Sep 2025 00:00:00 +0000</pubDate>
      <author>sciredomir@tutanota.com (Mahan)</author>
      <guid>https://aestheticvoyager.github.io/posts/resnet/</guid>
      <description>ResNet model and the seminal paper, &lt;em&gt;Deep Residual Learning for Image Recognition&lt;/em&gt; by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, which won the Best Paper award at CVPR 2016. It is one of the most influential and fundamental papers in the history of deep learning for computer vision.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://aestheticvoyager.github.io/posts/resnet/feature.jpg" />
    </item>
    
    <item>
      <title>VGGNet Overview</title>
      <link>https://aestheticvoyager.github.io/posts/vggnet/</link>
      <pubDate>Tue, 02 Sep 2025 00:00:00 +0000</pubDate>
      <author>sciredomir@tutanota.com (Mahan)</author>
      <guid>https://aestheticvoyager.github.io/posts/vggnet/</guid>
      <description>VGGNet is a famous deep learning model used in computer vision—essentially, teaching computers to understand images. It was created by researchers at the Visual Geometry Group (VGG) at the University of Oxford. Since its debut in 2014, VGGNet has become one of the key models that helped advance how machines see and recognize objects in photos. At its core, VGGNet is designed to look at images and decide what is in them.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://aestheticvoyager.github.io/posts/vggnet/feature.jpg" />
    </item>
    
  </channel>
</rss>
