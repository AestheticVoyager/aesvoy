<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on AesVoy</title>
    <link>http://localhost:1313/tags/ai/</link>
    <description>Recent content in AI on AesVoy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>© 2024 Mahan</copyright>
    <lastBuildDate>Thu, 08 Aug 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>From CNNs to Vision Transformers: The Future of Image Recognition</title>
      <link>http://localhost:1313/posts/vit/</link>
      <pubDate>Thu, 08 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/vit/</guid>
      <description>Vision Transformers (ViTs) are redefining image recognition by using Transformer models to capture global context, unlike traditional Convolutional Neural Networks (CNNs) that focus on local features. ViTs excel with large datasets and show impressive scalability and performance. However, they demand substantial data and computational power.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/vit/featured.png" />
    </item>
    
    <item>
      <title>imageNet-Computer Vision Backbone</title>
      <link>http://localhost:1313/posts/imagenet/</link>
      <pubDate>Tue, 06 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/imagenet/</guid>
      <description>ImageNet is more than just a dataset. The sheer scale of ImageNet, combined with its detailed labeling, made it essentially the backbone of Computer Vision.</description>
      
    </item>
    
    <item>
      <title>Transformers &amp; Attention</title>
      <link>http://localhost:1313/posts/attention-transformer/</link>
      <pubDate>Mon, 05 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/attention-transformer/</guid>
      <description>This blog post explains how self-attention and softmax function in Transformer models, crucial for modern NLP. It breaks down how self-attention helps models understand relationships between tokens and how softmax ensures efficient computation and numerical stability.</description>
      
    </item>
    
    <item>
      <title>Diffusion VS Auto-Regressive Models</title>
      <link>http://localhost:1313/posts/diffusion-vs-auto-regressive/</link>
      <pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/diffusion-vs-auto-regressive/</guid>
      <description>Generative AI has come a long way, producing stunning images from simple text prompts. But how do Diffusion and Auto-Regressive models work, and why are diffusion models preferred.</description>
      
    </item>
    
    <item>
      <title>AlexNet Revolution</title>
      <link>http://localhost:1313/posts/alexnet/</link>
      <pubDate>Fri, 26 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/alexnet/</guid>
      <description>In 2012, the field of artificial intelligence witnessed a seismic shift. The catalyst for this transformation was a deep learning model known as AlexNet.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/alexnet/featured.jpg" />
    </item>
    
    <item>
      <title>Generative Adversarial Network</title>
      <link>http://localhost:1313/posts/generative-adversarial-network/</link>
      <pubDate>Wed, 29 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/generative-adversarial-network/</guid>
      <description>A neural network is like a highly sophisticated, multi-layered calculator that learns from data. It consists of numerous “neurons” (tiny calculators) connected in layers, with each layer performing a unique function to help the network make predictions or decisions.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/generative-adversarial-network/featured.jpeg" />
    </item>
    
    <item>
      <title>Variational-Auto-Encoder</title>
      <link>http://localhost:1313/posts/variational-auto-encoder/</link>
      <pubDate>Tue, 28 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/variational-auto-encoder/</guid>
      <description>The beauty of VAEs lies in their ability to generate new samples by randomly sampling vectors from this known region and then passing them through the generator part of our model.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/variational-auto-encoder/featured.jpeg" />
    </item>
    
    <item>
      <title>Auto-Encoder</title>
      <link>http://localhost:1313/posts/auto-encoder/</link>
      <pubDate>Mon, 27 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/auto-encoder/</guid>
      <description>An autoencoder begins its journey by compressing input data into a lower dimension. It then endeavors to reconstruct the original input from this compressed representation.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/auto-encoder/featured.jpeg" />
    </item>
    
    <item>
      <title>Softmax</title>
      <link>http://localhost:1313/posts/softmax/</link>
      <pubDate>Fri, 19 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/softmax/</guid>
      <description>Softmax stands as a pivotal component in neural network architectures, offering a means to convert raw scores into interpretable probabilities.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/softmax/featured.png" />
    </item>
    
  </channel>
</rss>
