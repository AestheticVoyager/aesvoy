<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ML on AesVoy</title>
    <link>http://localhost:1313/tags/ml/</link>
    <description>Recent content in ML on AesVoy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>© 2024 Mahan</copyright>
    <lastBuildDate>Fri, 26 Jul 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/ml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AlexNet Revolution</title>
      <link>http://localhost:1313/posts/alexnet/</link>
      <pubDate>Fri, 26 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/alexnet/</guid>
      <description>In 2012, the field of artificial intelligence witnessed a seismic shift. The catalyst for this transformation was a deep learning model known as AlexNet.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/alexnet/featured.jpg" />
    </item>
    
    <item>
      <title>Generative Adversarial Network</title>
      <link>http://localhost:1313/posts/generative-adversarial-network/</link>
      <pubDate>Wed, 29 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/generative-adversarial-network/</guid>
      <description>A neural network is like a highly sophisticated, multi-layered calculator that learns from data. It consists of numerous “neurons” (tiny calculators) connected in layers, with each layer performing a unique function to help the network make predictions or decisions.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/generative-adversarial-network/featured.jpeg" />
    </item>
    
    <item>
      <title>Variational-Auto-Encoder</title>
      <link>http://localhost:1313/posts/variational-auto-encoder/</link>
      <pubDate>Tue, 28 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/variational-auto-encoder/</guid>
      <description>The beauty of VAEs lies in their ability to generate new samples by randomly sampling vectors from this known region and then passing them through the generator part of our model.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/variational-auto-encoder/featured.jpeg" />
    </item>
    
    <item>
      <title>Auto-Encoder</title>
      <link>http://localhost:1313/posts/auto-encoder/</link>
      <pubDate>Mon, 27 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/auto-encoder/</guid>
      <description>An autoencoder begins its journey by compressing input data into a lower dimension. It then endeavors to reconstruct the original input from this compressed representation.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/auto-encoder/featured.jpeg" />
    </item>
    
    <item>
      <title>Less is More Paper Review</title>
      <link>http://localhost:1313/posts/less-is-more/</link>
      <pubDate>Sun, 05 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/less-is-more/</guid>
      <description>Less is More: Parameter-Free Text Classification with Gzip offers a novel text classification method using gzip compression, eliminating manual parameter tuning.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/less-is-more/featured.jpeg" />
    </item>
    
    <item>
      <title>Infini-Attention Paper Review</title>
      <link>http://localhost:1313/posts/infini-attention/</link>
      <pubDate>Fri, 03 May 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/infini-attention/</guid>
      <description>Infini-Attention introduces a novel approach to scaling Transformer models for infinitely long inputs while managing memory and computation.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/infini-attention/featured.jpeg" />
    </item>
    
    <item>
      <title>Softmax</title>
      <link>http://localhost:1313/posts/softmax/</link>
      <pubDate>Fri, 19 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/softmax/</guid>
      <description>Softmax stands as a pivotal component in neural network architectures, offering a means to convert raw scores into interpretable probabilities.</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/posts/softmax/featured.png" />
    </item>
    
  </channel>
</rss>
